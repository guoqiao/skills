---
title: Systems Thinking
author: r4um
created_at: 2026-02-06T05:24:36.000Z
url: http://theprogrammersparadox.blogspot.com/2026/02/systems-thinking.html
points: 74
hn_url: https://news.ycombinator.com/item?id=46909439
comments: 8
---

# Systems Thinking

There are two main schools of thought in software development about how to build
really big, complicated stuff.

The most prevalent one, these days, is that you gradually evolve the complexity
over time. You start small and keep adding to it.

The other school is that you lay out a huge specification that would fully work
through all of the complexity in advance, then build it.

In a sense, it is the difference between the way an entrepreneur might approach
doing a startup versus how we build modern skyscrapers. Evolution versus
Engineering.

I was working in a large company a while ago, and I stumbled on the fact that
they had well over 3000 active systems that were covering dozens of lines of
business and all of the internal departments. It had evolved this way over fifty
years, and included lots of different tech stacks, as well as countless vendors.
Viewed as ‘one’ thing it was a pretty shaky house of cards.

It’s not hard to see that if they had a few really big systems, then a great
number of their problems would disappear. The inconsistencies between data,
security, operations, quality, and access were huge across all of those
disconnected projects. Some systems were up-to-date, some were ancient. Some
worked well, some were barely functional. With way fewer systems, a lot of these
self-inflicted problems would just go away.

It’s not that you could cut the combined complexity in half, but more likely
that you could bring it down to at least one-tenth of what it is today, if not
even better. It would function better, be more reliable, and would be far more
resilient to change. It would likely cost far less and require fewer employees
as well. All sorts of ugly problems that they have now would just not exist.

The core difference between the different schools really centers around how to
deal with dependencies.

If you had thousands of little blobs of complexity that were all entirely
independent, then getting finished is just a matter of banging out each one by
itself until they are all completed. That’s the dream.

But in practice, very few things in a big ecosystem are actually independent.
That’s the problem.

If you are going to evolve a system, then you ignore these dependencies. Sort
them out afterwards, as the complexity grows. It’s faster, and you can get
started right away.

If you were going to design a big system, then these dependencies dictate that
design. You have to go through each one and understand them all right away. They
change everything from the architecture all the way down to the idioms and style
in the code.

But that means that all of the people working to build up this big system have
to interact with each other. Coordinate and communicate. That is a lot of
friction that management and the programmers don’t want. They tend to feel like
it would all get done faster if they could just go off on their own. And it
will, in the short-term.

If you ignore a dependency and try to fix it later, it will be more expensive.
More time, more effort, more thinking. And it will require the same level of
coordination that you tried to avoid initially. Slightly worse, in that the time
pressures of doing it correctly generally give way to just getting it done
quickly, which pumps up the overall artificial complexity. The more hacks you
throw at it, the more hacks you will need to hold it together. It spirals out of
control. You lose big in the long-term.

One of the big speed bumps preventing big up-front designs is a general lack of
knowledge. Since the foundations like tech stacks, frameworks, and libraries are
always changing rapidly these days, there are few accepted best practices, and
most issues are incorrectly believed to be subjective. They’re not, of course,
but it takes a lot of repeated experience to see that.

The career path of most application programmers is fairly short. In most
enterprises, the majority have five years or less of real in-depth experience,
and battle-scared twenty-year+ vets are rare. Mostly, these novices are
struggling through early career experiences, not ready yet to deal with the
unbounded, massive complexity present in a big design.

Also, the other side of it is that evolutionary projects are just more fun. I’ve
preferred them. You’re not loaded down with all those messy dependencies. Way
fewer meetings, so you can just get into the work and see how it goes. Endlessly
arguing about fiddly details in a giant spec is draining, made worse if the
experience around you is weak.

Evolutionary projects go very badly sometimes. The larger they grow, the more
likely they will derail. And the fun gives way to really bad stress. That severe
last-minute panic that comes from knowing that the code doesn't really work as
it should, and probably never will. And the longer-term dissatisfaction of
having done all that work to ultimately just contribute to the problem, not
actually fix it.

Big up-front designs are often better from a stress perspective. A little slow
to start and sometimes slow in the middle, they mostly smooth out the overall
development process. You’ve got a lot of work to do, but you’ve also got enough
time to do it correctly. So you grind through it, piece by piece, being as
attentive to the details as possible. Along the way, you actively look for
smarter approaches to compress the work. Reuse, for instance, can shave a ton of
code off the table, cut down on testing, and provide stronger certainty that the
code will do the right thing in production.

The fear that big projects will end up producing the wrong thing is often
overstated. It’s true for a startup, but entirely untrue for some large business
application for a market that’s been around forever. You don’t need to burn a
lot of extra time, breaking the work up into tiny fragments, unless you really
don’t have a clue what you are building. If you're replacing some other existing
system, not only do you have a clue, you usually have a really solid long-term
roadmap. Replace the original work and fix its deficiencies.

There should be some balanced path in the middle somewhere, but I haven’t
stumbled across a formal version of it after all these decades.

We could go first to the dependencies, then come up with reasons why they can be
temporarily ignored. You can evolve the next release, but still have a vague big
design as a long-term plan. You can refactor the design as you come across new,
unexpected dependencies. Change your mind, over and over again, to try to get
the evolved works to converge on a solid grand design. Start fast, slow right
down, speed up, slow down again, and so forth. The goal is one big giant system
to rule them all, but it may just take a while to get there.

The other point is that the size of the iterations matters, a whole lot. If they
are tiny, it is because you are blindly stumbling forward. If you are not
blindly stumbling forward, they should be longer, as it is more effective. They
don’t have to all be the same size. And you really should stop and take stock
after each iteration. The faster people code, the more cleanup that is required.
The longer you avoid cleaning it up, the worse it gets, on basically an
exponential scale. If you run forward like crazy and never stop, the working
environment will be such a swamp that it will all grind to an abrupt stop. This
is true in building anything, or even cooking in a restaurant. Speed is a
tradeoff.

Evolution is the way to avoid getting bogged down in engineering, but
engineering is the way to ensure that the thing you build really does what it is
supposed to do. Engineering is slow, but spinning way out of control is a heck
of a lot slower. Evolution is obviously more dynamic, but it is also more
chaotic, and you have to continually accept that you’ve gone down a bad path and
need to backtrack. That is hard to admit sometimes. For most systems, there are
parts that really need to be engineered, and parts that can just be allowed to
evolve. The more random the evolutionary path, the more stuff you need to throw
away and redo. Wobbling is always expensive. Nature gets away with this by
having millions of species, but we really only have one development project, so
it isn’t particularly convenient.

--------------------------------------------------------------------------------
## Comments

readthenotes1: A major factor supporting evolution over big up-front design is
the drift in system requirements over time. Even on large military like
projects, apparently there's "discovery"--and the more years that pass, the more
requirements change.

    YZF: Even if the requirements are indeed fixed your understanding of the problem
    domain evolves.

    zppln: This isn't my experience. Requirements tend to settle over time (unless
    they're stupidly written). Users tend to like things to stay the same, with
    perhaps some improvement to performance here and there.  But if anything, all
    development is the search for the search for the requirements. Some just value
    writing them down.

bestham: “A complex system that works is invariably found to have evolved from a
simple system that worked. The inverse proposition also appears to be true: A
complex system designed from scratch never works and cannot be made to work. You
have to start over, beginning with a working simple system.” Gall’s Law

    YZF: So true.

    McGlockenshire: Ah, the Second System Effect, and the lesson learned from it.

        jeffreygoesto: But this is about the first systems?  I tend to tell people, the
        fourth try usually sticks.  The first is too ambitious and ends in an
        unmaintainable pile around a good core idea.  The second tries to "get
        everything right" and suffers second system syndrome.  The third gets it right
        but now for a bunch of central business needs. You learned after all. It is good
        exactly because it does not try to get _everything_ right like the second did.
        The fourth patches up some more features to scoop up B and C prios and calls it
        a day.  Sometimes, often in BigCorp: Creators move on and it will slowly
        deteriorate from being maintenaned...

    codeflo: This is often quoted, but I wonder whether it's actually strictly true,
    at least if you keep to a reasonable definition of "works". It's certainly not
    true in mechanical engineering.

    jackblemming: People misinterpret this and think they can incrementally build a
    skyscraper out of a shed.

    nasretdinov: I think the important part here is "from scratch". Typically when
    you're designing a new (second, third, whatever) system to replace the old one
    you actually take the good and the bad parts of the previous design into
    account, so it's no longer from scratch. That's what allows it to succeed (at
    least in my experience it usually did).

qwertyuiop_: Software cannot be built like skyscrapers because the sponsors know
about the malleability of the medium and treat it like a lump of clay that by
adding water can be shaped to something else.

    ako: You're mixing up design and manufacturing. A skyscraper is first completely
    designed (on paper, cad systems, prototypes), before it is manufactured. In
    software engineering, coding is often more a design phase than a manufacturing
    phase.  Designers need malleability, that is why they all want digital design
    systems.

    YZF: But software is in fact not very malleable at all. It's true the medium
    supports change, it's just a bunch of bits, but change is actually hard and
    expensive, perhaps more than other mediums.

        ako: With LLMs it's becoming very malleable.

        jbl0ndie: I'd argue it's more malleable than a skyscraper.  How rapidly has
        business software changed since COVID? Yet how many skyscrapers remain partially
        unoccupied in big cities like London, because of the recent arrival of
        widespread hybrid working?  The buildings are structurally unchanged and haven't
        been demolished to make way for buildings that better support hybrid working.
        Sure office fit outs are more oriented towards smaller simultaneous attendance
        with more hot desking. Also a new industry boom around team building socials has
        arrived. Virtual skeet shooting or golf, for example.  On the whole, engineered
        cities are unchanged, their ancient and rigid specifications lacking the
        foresight to include the requirements that accommodate hybrid working. Software
        meanwhile has adapted and as the OP says, evolved.

    baxtr: Funny you bring up the clay analogy.  It was discussed here just 2 days
    ago intensively.  https://news.ycombinator.com/item?id=46881543

ArchieScrivener: The Evolution method outlined also seems born from the
Continuous Delivery paradigm that was required for subscription business models.
I would argue Engineering is the superior approach as the Lean/Agile methods of
production were born from physical engineering projects whose end result was
complete. Evolution seems to be even more chaotic because an improper paradigm
of 'dev ops' was used instead of organically emerged as one would expect with an
evolving method.  Ai assistance would seem to favor the engineering approach as
the friction of teams and personalities is reduced in favor of quick feasibility
testing and complete planning.

iafan: > There are two main schools of thought in software development about how
to build really big, complicated stuff.  > The most prevalent one, these days,
is that you gradually evolve the complexity over time. You start small and keep
adding to it.  > The other school is that you lay out a huge specification that
would fully work through all of the complexity in advance, then build it.  I
think AI will drive an interesting shift in how people build software. We'll see
a move toward creating and iterating on specifications rather than
implementations themselves.  In a sense, a specification is the most compact
definition of your software possible. The knowledge density per "line" is much
higher than in any programming language. This makes specifications easier to
read, reason about, and iterate on—whether with AI or with peers.  I can imagine
open source projects that will revolve entirely around specifications, not
implementations. These specs could be discussed, with people contributing
thoughts instead of pull requests. The more articulated the idea, the higher its
chance of being "merged" into the working specification. For maintainers,
reviewing "idea merge requests" and discussing them with AI assistants before
updating the spec would be easier than reviewing code.  Specifications could be
versioned just like software implementations, with running versions and stable
releases. They could include addendums listing platform-specific caveats or
library recommendations. With a good spec, developers could build their own
tools in any language. One would be able to get a new version of the spec, diff
it with the current one and ask AI to implement the difference or discuss what
is needed for you personally and what is not. Similarly, It would be easier to
"patch" the specification with your own requirements than to modify ready-made
software.  Interesting times.

    simianwords: > I can imagine open source projects that will revolve entirely
    around specifications  This is a really good observation and I predict you will
    be correct.  There is a consequence of this for SaaS. You can imagine an example
    SaaS that one might need to vibecode to save money. The reason its not possible
    now is not because Claude can't do it, its because getting the right specs (like
    you suggested) is hard work. A well written spec will not only contain the best
    practices for that domain of software but also all the legal compliance BS that
    comes along with it.  With a proper specification that is also modular, I
    imagine we will be able to see more vibecoded SaaS.  Overall I think your
    prediction is really strong.

    energy123: Interested in ideas for this. I've mulled over different compact DSLs
    for specs, but unstructured (beyond file-specific ownership boundaries) has
    served me better.

        simianwords: I think it has to be modular and reusable. Like GDPR compliance
        spec should be opensourced and reused by all SaaS specs.

    _dark_matter_: Iceberg is, primarily, a spec [0]. It defines exactly what data
    is stored and how it is interacted with. The community debates broadly on spec
    changes first, see a recent one on cross-platform SQL UDFs [1].  We have yet to
    see a largely llm driven language implementation, but it is surely possible. I
    imagine it would be easier to tell the llm to instead translate the Java
    implementation to whatever language you need. A vibe-coded language could do
    major damage to a companies data.  [0] https://iceberg.apache.org/spec/ [1]
    https://lists.apache.org/thread/whbgoc325o99vm4b599f0g1owhgw...

        iafan: If I had a spec for something non-trivial, I probably would ask AI to
        create a test suite first. Or port tests from an existing system since each test
        is typically orders of magnitude easier to rewrite in any language, and then run
        AI in a loop until the tests pass.

    fc417fc802: There are parallels of thought here to template and macro libraries.
    One issue is that a spec without a working reference implementation is
    essentially the same as a pull request that's never been successfully compiled.
    Generalization is good but you can't get away from actually doing the thing at
    the end of the day.  I've run into this issue with C++ templates before. Throw a
    type at a template that it hasn't previously been tested with and it can fall
    apart in new and exciting ways.

    polyglotfacto: You can look at the Web as a starter:
    https://html.spec.whatwg.org/#history-2  > The WHATWG was based on several core
    principles, (..) and that specifications need to be detailed enough that
    implementations can achieve complete interoperability without reverse-
    engineering each other.  But in my experience you need more than a spec, because
    an implementation is not just something that implements a spec, it is also the
    result of making many architectural choices in how the spec is implemented.
    Also even with detailed specs AI still needs additional guidance. For example
    couple of weeks ago Cursor unleashed thousands of agents with access to web
    standards and the shared WPT test suite: the result was total nonsense.  So the
    future might rather be like a Russian doll of specs: start with a high-level
    system description, and then support it with finer-grained specs of parts of the
    system. This could go down all the way to the code itself: existing
    architectural patterns provide a spec for how to code a feature that is just a
    variation of such a pattern. Then whenever your system needs to do something
    new, you have to provide the code patterns for it. The AI is then relegated to
    its strength: applying existing patterns.  TLA+ has a concept of refinement,
    which is kind of what I described above as Russian dolls but only applied to
    TLA+ specs.  Here is a quote that describes the idea:  There is no fundamental
    distinction between specifications and implementations. We simply have
    specifications, some of which implement other specifications. A Java program can
    be viewed as a specification of a JVM (Java Virtual Machine) program, which can
    be viewed as a specification of an assembly language program, which can be
    viewed as a specification of an execution of the computer's machine
    instructions, which can be viewed as a specification of an execution of its
    register-transfer level design, and so on.  Source:
    https://cseweb.ucsd.edu/classes/sp05/cse128/ (chapter 1, last page)

praptak: Show me an example of a large complex software system built from spec
rather than evolved.

    iafan: Everything that is touching hardware, for example. Bluetooth stack, HDMI,
    you name it.  Everything W3C does. Go is evolving through specs first. Probably
    every other programming language these days.  People already do that for
    humankind-scale projects where there have to be multiple implementations that
    can talk to each other. Iteration is inevitable for anything that gains
    traction, but it still can be iteration on specs first rather than on code.

zkmon: > There should be some balanced path in the middle somewhere, but I
haven’t stumbled across a formal version of it after all these decades.  That's
very simple. The balanced path depends directly on how much of the requirements
and assumptions are going to change during the life time of the thing you are
building.  Engineering is helpful only to the extent you can forsee the future
changes. Anything beyond that requires evolution.  You are able to comment on
the complexity of that large company only because you are standing in the future
into 50 years from when those things started take shape. If you were designing
it 50 years back, you would end up with same complexity.  The nature's answer to
it is, consolidate and compact. Everything that falls onto earth gets compacted
into a solid rock over time, by a huge pressure of weight.  All complexity and
features are flattened out. Companies undergo similar dynamics driven by
pressures over time, not by big-bang engineering design upfront.

repelsteeltje: > Also, the other side of it is that evolutionary projects are
just more fun. I’ve preferred them. You’re not loaded down with all those messy
dependencies. Way fewer meetings, so you can just get into the work and see how
it goes. Endlessly arguing about fiddly details in a giant spec is draining,
made worse if the experience around you is weak.  IMO the problem isn't
discussing the spec per se. It's that the spec doesn't talk back the way actual
working code does. On a "big upfront design" project, there is a high chance
you're spending a lot of time on moot issues and irrelevant features.  Making a
good spec is much harder than making working software, because the spec may not
be right AND the spec may not describe the right thing.
