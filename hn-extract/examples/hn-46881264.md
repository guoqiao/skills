---
title: I miss thinking hard
author: jernestomg
created_at: 2026-02-04T03:54:11.000Z
url: https://www.jernesto.com/articles/thinking_hard
points: 619
hn_url: https://news.ycombinator.com/item?id=46881264
comments: 129
---

## Article

I miss thinking hard. Before you read this post, ask yourself a question: When
was the last time you truly thought hard? By “thinking hard,” I mean
encountering a specific, difficult problem and spending multiple days just
sitting with it to overcome it. a) All the time. b) Never. c) Somewhere in
between. If your answer is (a) or (b), this post isn't for you. But if, like me,
your response is (c), you might get something out of this, if only the feeling
that you aren't alone. First, a disclaimer: this post has no answers, not even
suggestions. It is simply a way to vent something I've been feeling for the last
few months. The Builder and The Thinker I believe my personality is built on two
primary traits: The Builder (The desire to create, ship, and be pragmatic). The
Thinker (The need for deep, prolonged mental struggle). The builder is pretty
self explanatory, it’s motivated by velocity and utility. It is the part of me
that craves the transition from “idea” to “reality.” It loves the dopamine hit
of a successful deploy, the satisfaction of building systems to solve real
problems, and the knowledge that someone, somewhere, is using my tool. To
explain the Thinker , I need to go back to my university days studying physics.
Every now and then, we would get homework problems that were significantly
harder than average. Even if you had a decent grasp of the subject, just coming
up with an approach was difficult. I observed that students fell into three
categories when facing these problems (well, four, if you count the 1% of
geniuses for whom no problem was too hard). Type 1: The majority. After a few
tries, they gave up and went to the professor or a TA for help. Type 2: The
Researchers. They went to the library to look for similar problems or insights
to make the problem approachable. They usually succeeded. Type 3: The Thinkers.
I fell into the third category, which, in my experience, was almost as rare as
the genius 1%. My method was simply to think. To think hard and long. Often for
several days or weeks, all my non-I/O brain time was relentlessly chewing on
possible ways to solve the problem, even while I was asleep. This method never
failed me. I always felt that deep prolonged thinking was my superpower. I might
not be as fast or naturally gifted as the top 1%, but given enough time, I was
confident I could solve anything. I felt a deep satisfaction in that process.
The Conflict with AI That satisfaction is why software engineering was initially
so gratifying. It hit the right balance. It satisfied The Builder (feeling
productive and pragmatic by creating useful things) and The Thinker (solving
really hard problems). Thinking back, the projects where I grew the most as an
engineer were always the ones with a good number of really hard problems that
needed creative solutions. But recently, the number of times I truly ponder a
problem for more than a couple of hours has decreased tremendously. Yes, I blame
AI for this. I am currently writing much more, and more complicated software
than ever, yet I feel I am not growing as an engineer at all. When I started
meditating on why I felt “stuck,” I realized I am starving The Thinker. “Vibe
coding” satisfies the Builder. It feels great to see to pass from idea to
reality in a fraction of a time that would take otherwise. But it has
drastically cut the times I need to came up with creative solutions for
technical problems. I know many people who are purely Builders, for them this
era is the best thing that ever happened. But for me, something is missing. The
Trap of Pragmatism I know what you might be thinking: "If you can ‘vibe code’
your way through it, the problem wasn’t actually hard." I think that misses the
point. It’s not that AI is good for hard problems, it’s not even that good for
easy problems. I’m confident that my third manual rewrite of a module would be
much better than anything the AI can output. But I am also a pragmatist. If I
can get a solution that is “close enough” in a fraction of the time and effort,
it is irrational not to take the AI route. And that is the real problem: I
cannot simply turn off my pragmatism. At the end of the day, I am a Builder. I
like building things. The faster I build, the better. Even if I wanted to reject
AI and go back to the days where the Thinker's needs were met by coding, the
Builder in me would struggle with the inefficiency. Even though the AI almost
certainly won't come up with a 100% satisfying solution, the 70% solution it
achieves usually hits the “good enough” mark. So, what now? To be honest, I
don’t know. I am still figuring it out. I'm not sure if my two halves can be
satisfied by coding anymore. You can always aim for harder projects, hoping to
find problems where AI fails completely. I still encounter those occasionally,
but the number of problems requiring deep creative solutions feels like it is
diminishing rapidly. I have tried to get that feeling of mental growth outside
of coding. I tried getting back in touch with physics, reading old textbooks.
But that wasn’t successful either. It is hard to justify spending time and
mental effort solving physics problems that aren’t relevant or state-of-the-art
when I know I could be building things. My Builder side won’t let me just sit
and think about unsolved problems, and my Thinker side is starving while I vibe-
code. I am not sure if there will ever be a time again when both needs can be
met at once. - Philipp Mainländer

--------------------------------------------------------------------------------

## Comments

Besibeta: The problem with the "70% solution" is that it creates a massive
amount of hidden technical debt. You aren't thinking hard because you aren't
forced to understand the edge cases or the real origin of the problem. It used
to be the case that you will need plan 10 steps ahead because refactoring was
expensive, now people just focus in the next problem ahead, but the compounding
AI slop will blow up eventually.

    andsoitis: would you agree that there's more time to think about what problems
    are worth solving?

joshpicky: I generally feel the same. But in addition, I also enjoy the pure act
of coding. At least for me that’s another big part why I feel left behind with
all this Agent stuff.

    jernestomg: I agree, that's another factor. Definitely the mechanical act of
    coding specially if your are good at it gives the type of joy that I can imagine
    an artisan or craftsman having when doing his work.

r-johnv: I've found that it's often useful to spend the time thinking about the
way I would architect the code (down to a fair level of minutia) before letting
the agent have a go. That way my 'thinker' is satiated and also challenged - Did
the solution that my thinker came up with solve the problem better than the plan
that the agent wrote? Then either I acknowledge that the agent's solution was
better, giving my thinker something to chew on for the next time; or my solution
is better which gives the thinker a dopamine hit and gives me better code.

bigstrat2003: Dude, I know you touched on this but seriously. Just don't use AI
then. It's not hard, it's your choice to use it or not. It's not even making you
faster, so the pragmatism argument doesn't really work well! This is a totally
self inflicted problem that you can undo any time you want.

    donatj: Spoken like someone who doesn't have their company measuring their AI
    usage and regularly laying people off.

        Aeolun: Need to be in the top 5% of AI users while staying in your budget of
        $50/month!

        CuriouslyC: It's not hard to burn tokens on random bullshit (see moltbook). If
        you really can deliver results at full speed without AI, it shouldn't be hard to
        keep cover.

        llmthrow0827: If you can't figure out how to game this, you're both not thinking
        hard and not using AI effectively.

        renewiltord: I have a Claude code set up in a folder with instructions on how to
        access iMessage. Ask it questions like “What did my wife say I should do next
        Friday?” Reads the SQLite db and shit. So burn your tokens on that.

        layer8: That sucks, but honestly I’d get out of there as fast as possible. Life
        is too short to live under unfulfilling work conditions for any extended amount
        of time.

topspin: I'm using LLMs to code and I'm still thinking hard.  I'm not doing it
wrong: I think about design choices: risks, constraints, technical debt,
alternatives, possibilities...  I'm thinking as hard as I've ever done.

    paladin314159: I echo this sentiment. Even though I'm having Claude Code write
    100% of the code for a personal project as an experiment, the need for thinking
    hard is very present. In fact, since I don't need to do low-thinking tasks like
    writing boilerplate or repetitive tests, I find my thinking ratio is actually
    higher than when I write code normally.

        jernestomg: I'm with you, thinking about architecture is generally still  a big
        part of my mental effort. But for me most architectural problems are solve in
        short periods of thought  and a lot of iteration. Maybe its an skill issue, but
        not now nor in the pre-LLM era I've been able to pre-solve all the architecture
        with pure thinking. That said architectural problems have been also been less
        difficult, just for the simple fact that research and prototyping has become
        faster and cheaper.

            ratorx: I think it depends on the scope and level of solution I accept as
            “good”. I agree that often the thinking for the “next step” is too easy
            architecturally. But I still enjoy thinking about the global optimum or a
            “perfect system”, even it’s not immediately feasible, and can spend large
            amounts of time on this. And then also there’s all the non-systems stuff - what
            is actually feasible, what’s most valuable etc. Less “fun”, but still lots of
            potential for thinking. I guess my main point is there is still lots to think
            about even post-LLM, but the real challenge is making it as “fun” or as easily
            useful as it was pre-LLM. I think local code architecture was a very easy domain
            for “optimality” that is actually tractable and the joy that comes with it, and
            LLMs are harmful to that, but I don’t think there’s nothing to replace it with.

        Aeolun: And thinking of how to convey all of that to Claude without having to
        write whole books :)

            MarcelOlsz: tfw you start expressing your thoughts as code because its shorter
            instead

                sodapopcan: Ya, they are programming languages after all.  Language is really
                powerful when you really how to use it.  Some of us are more comfortable with
                the natural variety, some of us are more comfy with code ¯\_(ツ)_/¯

        exodust: Agreed. My recent side projects involve lots of thinking over days and
        weeks. With AI we can set high bars and do complex original stuff. Obviously
        boilerplate and  common patterns are slop slap without much thinking. That's why
        you branch into new creative territory.  The challenge then becomes visualising
        the mental map of modular pieces all working nicely together at the right time
        to achieve your original intent.

    wnolens: Yes, if anything I think harder because I know it's on the frontier of
    whatever I'm building (so i'm more motivated and there's much more ROI)

    amiantos: I use Claude Code a lot, and it always lets me know the moment I
    stopped thinking hard, because it will build something completely asinine.
    Garbage in, garbage out, as they say...

    holysoles: I very much think its possible to use LLMs as a tool in this way.
    However a lot of folks are not. I see people, both personally and
    professionally, give it a problem and expect it to both design and implement a
    solution, then hold it as a gold standard. I find the best uses, for at least my
    self, are smaller parts of my workflow where I'm not going to learn anything
    from doing it: - build one to throw away: give me a quick prototype to get
    stakeholder feedback - straightforward helper functions: I have the design and
    parameters planned, just need an implementation that I can review - tab-
    completion code-gen - If I want leads for looking into something (libraries,
    tools) and Googling isn't cutting it

        topspin: > then hold it as a gold standard I just changed employers recently in
        part due to this: dealing with someone that appears to now spend his time
        coercing LLM's to give the answers he wants, and becoming deaf to any
        contradictions.  LLMs are very effective at amplifying the Reality Distortion
        Field for those that live in them.  LLMs are replacing blog posts for this
        purpose.

    gkoberger: I'd go as far as to say I think harder now – or at least quicker. I'm
    not wasting cycles on chores; I can focus on the bigger picture.

        9rx: I've never felt more mental exhaustion than after a LLM coding session. I
        assume that is a result of it requiring me to think harder too.

            josephg: I feel this too. I suspect its a byproduct of all the context switching
            I find myself doing when I'm using an LLM to help write software. Within a 10
            minute window, I'll read code, debug a problem, prompt, discuss the design, test
            something, do some design work myself and so on. When I'm just programming, I
            spend a lot more time working through a single idea, or a single function. Its
            much less tiring.

            AlotOfReading: It wasn't until I read your comment that I was able to pinpoint
            why the mental exhaustion feels familiar. It's the same kind (though not degree)
            of exhaustion as formal methods / proofs. Except without the reward of an
            intellectual high afterwards.

                samusiam: Personally I do get the intellectual high after a long LLM coding
                session.

            Gigachad: In my experience it's because you switch from writing code to
            reviewing code someone else wrote. Which is massively more difficult than
            writing code yourself.

    thrw045: Reading this comment and other similar comments there's definitely a
    difference between people.  Personally I agree and resonate a lot with the blog
    post, and I've always found designs of my programs to come sort of naturally.
    Usually the hard problems are the technical problems and then the design is
    figured out based on what's needed to control the program. I never had to think
    that hard about design.

        cwnyth: Aptitude testing centers like Johnson O'Connor have tests for that.
        There are (relatively) huge differences between different people's thinking and
        problem solving styles. For some, creating an efficient process feels natural,
        while others need stability and redundancy. Programmers are by and large the
        latter. [1]: https://www.jocrf.org/how-clients-use-the-analytical-reasoni...

    Aeglaecia: there's no such thing as right or wrong , so the following isn't
    intended as any form of judgement or admonition , merely an observation that you
    are starting to sound like an llm

        topspin: > you are starting to sound like an llm My observation: I've always had
        that "sound."  I don't know or care much about what that implies.  I will admit
        I'm now deliberately avoiding em dashs, whereas I was once an enthusiastic user
        of them.

            samusiam: I still use em-dashes. I started using them when my professor
            lambasted my use of semi-colons. I'm not looking back -- LLM haters be damned!

    johnfn: It's certainly a different style of thinking hard. I used to really
    stress myself over coding - i.e. I would get frustrated that solving an issue
    would cause me to introduce some sort of hack or otherwise snowball into a huge
    refactor. Now I spend most of my time thinking about what cool new features I am
    going to build and not really stressing myself out too much.

    sho_hn: I think OP's post is an attempt to move us past this stage of the
    discussion, which is frankly an old hat. The point they are making is that using
    AI tools makes it a lot harder for them to keep up the discipline to think hard.
    This may or may not be true for everyone.

    ksymph: It is a different kind of thinking, though.

    josephg: Yeah, but thinking with an LLM is different. The article says: > By
    “thinking hard,” I mean encountering a specific, difficult problem and spending
    multiple days just sitting with it to overcome it. The "thinking hard" I do with
    an LLM is more like management thinking. Its chaotic and full of conversations
    and context switches. Its tiring, sure. But I'm not spending multiple days
    contemplating a single idea. The "thinking hard" I do over multiple days with a
    single problem is more like that of a scientist / mathematician. I find myself
    still thinking about my problem while I'm lying in bed that night. I'm
    contemplating it in the shower. I have little breakthroughs and setbacks, until
    I eventually crack it or give up. Its different.

        marcus_holmes: There are a lot of hard problems to solve in orchestration. We've
        barely scratched the surface on this.

        buu700: YMMV, but I've found that I actually do way more of that type of
        "thinking hard" thanks to LLMs. With the menial parts largely off my plate, my
        attention has been freed up to focus on a higher density of hard problems, which
        I find a lot more enjoyable.

    senectus1: its how you use the tool... reminds me of that episode of simpsons
    when homer gets a gun lic... he goes from not using it at all, to using it a
    little, to using it without thinking about what hes doing and for ludicrous
    things... thinking is tiring and life is complicated, the tool makes it easy to
    slip into bad habits and bad habits are hard to break even when you recognise
    its a bad habit. Many people are too busy/lazy/self-unaware to evaluate their
    behaviour to recognise a bad habit.

    allovertheworld: Thats not thinking hard, you are making decisions

    lelanthran: > I'm using LLMs to code and I'm still thinking hard. I'm not doing
    it wrong: I think about design choices: risks, constraints, technical debt,
    alternatives, possibilities... I'm thinking as hard as I've ever done. Okay, for
    you that is new - post-LLM. For me, pre-LLM I thought about all those things as
    well as the code itself. IOW, I thought about even more things. Now you (if I
    understand your claim correctly) think only about those higher level things,
    unecumbered by stuff like implementation misalignments, etc. By definition
    alone, you are thinking less hard. ------------------------ [1] Many times the
    thinking about code itself acted as a feedback mechanism for all those things.
    If thinking about the code itself never acted as a feedback mechanism to your
    higher thought processes then ... well, maybe you weren't doing it the way I
    was.

hahahahhaah: I think AI didn't do this. Open source, libraries, cloud,
frameworks and agile conspired to do this. Why solve a problem when you can
import library / scale up / use managed kuberneted / etc. The menu is great and
the number of problems needing deep thought seems rare. There might be deep
thought problems on the requirements side of things but less often on the
technical side.

oa335: I feel like AI has given me the opportunity to think MORE, not less.  I’m
doing so much less mindless work, spending most of my efforts critically
analyzing the code and making larger scale architectural decisions. The author
says “ Even though the AI almost certainly won't come up with a 100% satisfying
solution, the 70% solution it achieves usually hits the “good enough” mark.” The
key is to keep pushing until it gets to the 100% mark. That last 30% takes
multiples longer than the first 70%, but that is where the satisfaction lies for
me.

zatkin: I feel that AI doesn't necessarily replace my thinking, but actually
helps to explore deeper - on my behalf - alternative considerations in the
approach to solving a problem, which in turn better informs my thinking.

sergiotapia: With AI, I now think much harder. Timelines are shorter, big
decisions are closer together, and more system interactions have to be "grokked"
in my head to guide the model properly. I'm more spent than before where I would
spend 2 hours wrestling with tailwind classes, or testing API endpoints manually
by typing json shapes myself.

ChaitanyaSai: I miss the thrill of running through the semi-parched grasslands
and the heady mix of terror triumph and trepidation as we close in on our meal
for the week.

    Aeolun: I think that feeling is fairly common across the entire population. Play
    more tag, it’ll help.

    goatlover: There are people who still hunt, fish and run. Some even climb
    without ropes. It would seem the feeling is missed.

keithnz: I feel like I'm doing much nicer thinking now, I'm doing more systems
thinking, not only that I'm iterating on system design a lot more because it is
a lot easier to change with AI

Fire-Dragon-DoL: I haven't reduced my thinking! Today I asked AI to debug an
issue. It came with a solution that it was clearly correct, but it didn't
explain why the code was in that state. I kept steering AI (which just wanted to
fix) toward figuring out the why and it digged through git and github issue at
some point,in a very cool way. And finally it pulled out something that made
sense. It was defensive programming introduced to fix an issue somewhere else,
which was also in turn fixed, so useless. At that point an idea popped in my
mind and I decided to look for similar patterns in the codebase, related to the
change, found 3. 1 was a non bug, two were latent bugs. Shipped a fix plus 2
fixes for bugs yet to be discovered.

    throwerxyz: >I haven't reduced my thinking! You just detailed an example of
    where you did in fact reduce your thinking. Managers who tell people what to get
    done do not think about the problem.

        Fire-Dragon-DoL: I think my message is doing a disservice to explaining what
        actually happened because a lot of it happens in my head.     1. I received the
        ticket, as soon as I read it I had a hunch it was related to some querying
        ignoring a field that should be filtered by every query (thinking)     2. I give
        this hunch to the AI which goes search in the codebase in the areas I suggested
        the problem could be and that's when it find the issue and provide a fix     3.
        I think the problem could be spread given there is a method that removes the
        query filter, it could have been used in multiple places, so I ask AI to find
        other usages of it (thinking, this is my definition of "steering" in this
        context)     4. AI reports 3 more occurrences and suggests that 2 have the same
        bug, but one is ok     5. I go in, review the code and understand it and I
        agree, it doesn't have the bug (thinking)     6. AI provide the fix for all the
        right spots, but I said "wait, something is fishy here, there is a commit that
        explicitly say it was added to remove the filter, why is that?" (thinking), so I
        ask AI to figure out why the commit says that     7. AI proceeds to run a bunch
        of git-history related commands, finds some commit and then does some
        correlation to find another commit. This other commit introduced the change at
        the same time to defend from a bug in a different place    8. I understand
        what's going on now, I'm happy with the fix, the history suggests I am not
        breaking stuff. I ask AI to write a commit with detailed information about the
        bug and the fix based on the conversation       There is a lot of thinking
        involved. What's reduced is search tooling. I can be way more fuzzy, rather than
        `rg 'whatever'` I now say "find this and similar patterns"

            booleandilemma: Did you use your AI to create that list for you?

                phist_mcgee: That's not very nice. Be nice.

            glemmaPaul: Thanks for expanding your comment. But to what you explain here, I
            think your knowledge and comprehension has only slimmed down a notch. It seems
            to me that this argument equates thinking to be on the vertical vertices only,
            but may I say there is a horizontal/broad aspect to it? e.g. You lose grip on
            what is a good combination of framework/language/standards, you remove the
            abstraction of multiple layers of external and internal APIs, you leave to study
            the right software pattern for the job, having the AI comprehend the large
            chunks for you (thats all loss on thinking). You've lost simple querying and
            digging through codebase. Gosh, lets even say you lost a bit of git command
            knowledge. You catch my drift here? I am completely for using AI as a tool to do
            a lot of the boilerplate work with the right directions. Though remembering some
            changes in codebase before and letting LLMs do the work, is not the same to me
            as fully owning up to your system as you know, you actually know. Old man
            shouting at screen so, to each their own of course! Cheers

porcoda: > At the end of the day, I am a Builder. I like building things. The
faster I build, the better. This I can’t relate to.  For me it’s “the better I
build, the better”.  Building poor code fast isn’t good: it’s just creating debt
to deal with in the future, or admitting I’ll toss out the quickly built thing
since it won’t have longevity.  When quality comes into play (not just “passed
the tests”, but is something maintainable, extensible, etc), it’s hard to not
employ the Thinker side along with the Builder.  They aren’t necessarily
mutually exclusive. Then again, I work on things that are expected to last quite
a while and aren’t disposable MVPs or side projects.  I suppose if you don’t
have that longevity mindset it’s easy to slip into Build-not-Think mode.

AdieuToLogic: Cognitive skills are just like any other - use them and they will
grow, do not and they will decline.  Oddly enough, the more one increases their
software engineering cognition, the less the distance between "The Builder" and
"The Thinker" becomes.

raincole: I really don't believe AI allows you to think less hard. If it did, it
would be amazing, but the current AI hasn't got to that capability. It forces
you to think about different things at best.

rustystump: At the day job there was a problem with performance loading data in
an app. 7 months later waffling on it on and off with and without ai I finally
cracked it. Author is not wrong though, the number of times i hit this isnt as
often since ai. I do miss the feeling though

Dr_Birdbrain: I think this problem existed before AI. At least in my current
job, there is constant, unrelenting demand for fast results. “Multi-day deep
thinking” sounds like an outrageous luxury, at least in my current job.

    renegade-otter: Which is a reason for software becoming worse across the board.
    Just look at Windows. The "go go go" culture is ruinous to products.

        cpncrunch: Even 30 years ago when I started in the industry, most jobs required
        very little deep thinking. All of mine has been done on personal projects. Thats
        just the reality of the typical software engineering job.

    tbmtbmtbmtbmtbm: this is why productivity is a word that should really just be
    reserved for work contexts, and personal time is better used for feeding "The
    Thinker"

gyomu: This March 2025 post from Aral Balkan stuck with me:
https://mastodon.ar.al/@aral/114160190826192080 "Coding is like taking a lump of
clay and slowly working it into the thing you want it to become. It is this
process, and your intimacy with the medium and the materials you’re shaping,
that teaches you about what you’re making – its qualities, tolerances, and
limits – even as you make it. You know the least about what you’re making the
moment before you actually start making it. That’s when you think you know what
you want to make. The process, which is an iterative one, is what leads you
towards understanding what you actually want to make, whether you were aware of
it or not at the beginning. Design is not merely about solving problems; it’s
about discovering what the right problem to solve is and then solving it. Too
often we fail not because we didn’t solve a problem well but because we solved
the wrong problem. When you skip the process of creation you trade the thing you
could have learned to make for the simulacrum of the thing you thought you
wanted to make. Being handed a baked and glazed artefact that approximates what
you thought you wanted to make removes the very human element of discovery and
learning that’s at the heart of any authentic practice of creation. Where you
know everything about the thing you shaped into being from when it was just a
lump of clay, you know nothing about the image of the thing you received for
your penny from the vending machine."

    anonymous344: yes, this is maybe it's my preference to jump directly to coding,
    instead of canva to draw the gui and stuff. i would not know what to draw
    because the involvemt is not so deep ...or something

    helloplanets: And when programming with agentic tools, you need to actively push
    for the idea to not regress to the most obvious/average version. The amount of
    effort you need to expend on pushing the idea that deviates from the 'norm'
    (because it's novel), is actually comparable to the effort it takes to type
    something out by hand. Just two completely different types of effort. There's an
    upside to this sort of effort too, though. You actually need to make it crystal
    clear what your idea is and what it is not, because of the continuous pushback
    from the agentic programming tool. The moment you stop pushing back, is the
    moment the LLM rolls over your project and more than likely destroys what was
    unique about your thing in the first place.

        fallous: You just described the burden of outsourcing programming.

            tomrod: 100%! There is significant analogy between the two!

                salawat: There is a reason management types are drawn to it like flies to shit.

                    theshrike79: Working with and communicating with offshored teams is a specific
                    skill too. There are tips and tricks on how to manage them and not knowing them
                    will bite you later on. Like the basic thing of never asking yes or no
                    questions, because in some cultures saying "no" isn't a thing. They'll rather
                    just default to yes and effectively lie than admit failure.

            darkwater: With the basic and enormous difference that the feedback loop is 100
            or even 1000x faster. Which changes the type of game completely, although other
            issues will probably arise as we try this new path.

                Terr_: That embeds an assumption that the outsourced human workers are incapable
                of thought, and experience/create zero feedback loops of their own. Frustrated
                rants about deliverables aside, I don't think that's the case.

                    darkwater: No. It just means the harsh reality: what's really soul crushing in
                    outsourced work is having endless meetings to pass down / get back information,
                    having to wait days/weeks/months to get some "deliverable" back on which iterate
                    etc.  Yes, outsourced human workers are totally capable of creative thinking
                    that makes sense, but their incentive will always be throughput over quality,
                    since their bosses usually give closed prices (at least in what I lived
                    personally). If you are outsourcing to an LLM in this case YOU are still in
                    charge of the creative thought. You can just judge the output and tune the
                    prompts or go deep in more technical details and tradeoffs. You are "just" not
                    writing the actual code anymore, because another layer of abstraction has been
                    added.

            agumonkey: We need a new word for on-premise offshoring. On-shoring ;

                aleph_minus_one: > On-shoring I thought "on-shoring" is already commonly used
                for the process that undos off-shoring.

                    saghm: How about "in-shoring"? We already have "insuring" and "ensuring", so we
                    might as well add another confusingly similar sounding term to our vocabulary.

                intended: Ai-shoring. Tech-shoring.

                    johnisgood: Would work, but with "snoring". :D

                    dzdt: vibe-shoring

                pferde: Corporate has been using the term "best-shoring" for a couple of years
                now. To my best guess, it means "off-shoring or on-shoring, whichever of the two
                is cheaper".

            onion2k: Outsourcing development and vibe coding are incredibly similar
            processes. If you just chuck ideas at the external coding team/tool you often
            get rubbish back. If you're good at managing the requirements and defining
            things well you can achieve very good things with much less cost.

        jiveturkey: > need to make it crystal clear That's not an upside in that it's
        unique to LLM vs human written code. When writing it yourself, you also need to
        make it crystal clear. You do that in the language of implementation.

            balamatom: And programming languages are designed for clarifying the
            implementation details of abstract processes; while human language is this
            undocumented, half grandfathered in, half adversarially designed instrument for
            making apes get along (as in, move in the same general direction) without
            excessive stench. The humane and the machinic need to meet halfway - any
            computing endeavor involves not only specifying something clearly enough for a
            computer to execute it, but also communicating to humans how to benefit from the
            process thus specified. And that's the proper domain not only of software
            engineering, but the set of related disciplines (such as the various non-coding
            roles you'd have in a project team - if you have any luck, that is). But
            considering the incentive misalignments which easily come to dominate in this
            space even when multiple supposedly conscious humans are ostensibly keeping
            their eyes on the ball, no matter how good the language machines get at doing
            the job of any of those roles, I will still intuitively mistrust them exactly as
            I mistrust any human or organization with responsibly wielding the kind of pre-
            LLM power required for coordinating humans well enough to produce industrial-
            scale LLMs in the first place. What's said upthread about the wordbox
            continually trying to revert you to the mean as you're trying to prod it with
            the cowtool of English into outputting something novel, rings very true to me.
            It's not an LLM-specific selection pressure, but one that LLMs are very likely
            to have 10x-1000xed as the culmination of a multigenerational gambit of sorts;
            one whose outset I'd place with the ever-improving immersive simulations that
            got the GPU supply chain going.

        GCUMstlyHarmls: I can't help but imagine training horses vs training cats. One
        of them is rewarding, a pleasure, beautiful to see, the other is frustrating,
        leaves you with a lot of scratches and ultimately both of you "agreeing" on a
        marginal compromise.

            lambdaone: Right now vibe coding is more like training cats. You are constantly
            pushing against the model's tendency to produce its default outputs regardless
            of your directions. When those default outputs are what you want - which they
            are in many simple cases of effectively English-to-code translation with
            memorized lookup - it's great. When they are not, you might as well write the
            code yourself and at least be able to understand the code you've generated.

        Der_Einzige: Yet another example of "comments that are only sort of true because
        high temperature sampling isn't allowed". If you use LLMs at very high
        temperature with samplers which correctly keep your writing coherent (i.e.
        Min_p, or better like top-h, P-less decoding, etc), than "regression to the
        mean" literally DOES NOT HAPPEN!!!!

            adevilinyc: How do you configure LLM température in coding agents, e.g.
            opencode?

                Der_Einzige: You can't without hacking it! That's my point! The only places you
                can easily are via the API directly, or "coomer" frontends like SillyTavern,
                Oobabooga, etc. Same problem with image generation (lack of support for
                different SDE solvers, the image version of LLM sampling) but they have
                different "coomer" tools, i.e. ComfyUI or Automatic1111

                    yoyohello13: Once again, porn is where the innovation is…

                        dizhn: Please.. "Creative Writing"

                kabr: https://opencode.ai/docs/agents/#temperature set it in your opencode.json

            hnlmorg: Have you actually tried high temperature values for coding? Because I
            don’t think it’s going to do what you claim it will. LLMs don’t “reason” the
            same way humans do. They follow text predictions based on statistical relevance.
            So raising the temperature will more likely increase the likelihood of
            unexecutable pseudocode than it would create a valid but more esoteric
            implementation of a problem.

                Terr_: To put it another way, a high-temperature mad-libs machine will write a
                very unusual story, but that isn't necessarily the same as a clever story.

                    balamatom: So why is this "temperature" not on, like, a rotary encoder? So you
                    can just, like, tweak it when it's working against your intent in either
                    direction?

                bob1029: High temperature seems fine for my coding uses on GPT5.2. Code that
                fails to execute or compile is the default expectation for me. That's why we
                feed compile and runtime errors back into the model after it proposes something
                each time. I'd much rather the code sometimes not work than to get stuck in
                infinite tool calling loops.

        dkdbejwi383: Fair enough but I am a programmer because I like programming. If I
        wanted to be a product manager I could have made that transition with or without
        LLMs.

        fflluuxx: This is why people thinkless of artists like Damien Hirst and Jeff
        Koons because their hands have never once touched the art. They have no
        connection to the effort. To the process. To the trail and error. To the suffer.
        They’ve out sourced it, monetized it, and make it as efficient as possible. It’s
        also soulless.

    boredtofears: I dunno, when you've made about 10,000 clay pots its kinda nice to
    skip to the end result, you're probably not going to learn a ton with clay pot
    #10,001. You can probably come up with some pretty interesting ideas for what
    you want the end result to look like from the onset. I find myself being able to
    reach for the things that my normal pragmatist code monkey self would consider
    out of scope - these are often not user facing things at all but things that
    absolutely improve code maintenance, scalability, testing/testability, or reduce
    side effects.

        belZaah: Depends on the problem. If the complexity of what you are solving is in
        the business logic or, generally low, you are absolutely right. Manually coding
        a signup flow #875 is not my idea of fun either. But if the complexity is in the
        implementation, it’s different. Doing complex cryptography, doing performance
        optimization or near-hardware stuff is just a different class of problems.

            boredtofears: In my experience AI is pretty good at performance optimizations as
            long as you know what to ask for. Can't speak to firmware code or complex
            cryptography but my hunch is if it's in it's training dataset and you know
            enough to guide it, it's generally pretty useful.

                kranner: > my hunch is if it's in it's training dataset and you know enough to
                guide it, it's generally pretty useful. Presumably humanity still has room to
                grow and not everything is already in the training set.

                aleph_minus_one: >  In my experience AI is pretty good at performance
                optimizations as long as you know what to ask for. This rather tells that the
                kind of performance optimizations that you ask for are very "standard".

                    charcircuit: Most optimizations are making sure you do not do work that is
                    unnecessary or that you use the hardware effectively. The standard techniques
                    are all you need 99% of the time you are doing performance work. The hard part
                    about performance is dedicating the time towards it and not letting it regress
                    as you scale the team. With AI you can have agents constantly profiling the
                    codebase identifying and optimizing hotspots as they get introduced.

            aleph_minus_one: > If the complexity of what you are solving is in the business
            logic or, generally low, you are absolutely right. The problem is rather that
            programmers who work on business logic often hate programmers who are actually
            capable of seeing (often mathematical) patterns in the business logic that could
            be abstracted away; in other words: many business logic programmers hate
            abstract mathematical stuff. So, in my opinion/experience this is a very self-
            inflected problem that arises from the whole culture around business logic and
            business logic programming.

        bravetraveler: import claypot trillion dollar industry boys

    CamperBob2: Eloquent, moving, and more-or-less exactly what people said when
    cameras first hit the scene.

        vermilingua: Source?

            CamperBob2: Art history.  It's how we ended up with Impressionism, for instance.
            People felt (wrongly) that traditional representational forms like portraiture
            were threatened by photography.  Happily, instead of killing any existing
            genres, we got some interesting new ones.

        AdieuToLogic: > Eloquent, moving, and more-or-less exactly what people said when
        cameras first hit the scene. This is a non sequitur. Cameras have not replaced
        paintings, assuming this is the inference.  Instead, they serve only to be an
        additional medium for the same concerns quoted:   The process, which is an
        iterative one, is what leads you    towards understanding what you actually want
        to make,    whether you were aware of it or not at the beginning.  Just as this
        is applicable to refining a software solution captured in code, just as a
        painter discards unsatisfactory paintings and tries again, so too is it when
        people say, "that picture didn't come out the way I like, let's take another
        one."

            CamperBob2: Cameras have not replaced paintings, assuming this is the inference.
            You wouldn't have known that, going by all the bellyaching and whining from the
            artists of the day. Guess what, they got over it.  You will too.

                kranner: > Guess what, they got over it. You will too. Prediction is difficult,
                especially of the future.

                AdieuToLogic: >> Cameras have not replaced paintings, assuming this is the
                inference. > You wouldn't have known that, going by all the bellyaching and
                whining from the artists of the day. > Guess what, they got over it. You
                conveniently omitted my next sentence, which contradicts your position and reads
                thusly:   Instead, they serve only to be an additional medium for the    same
                concerns quoted ...  > You will too. This statement is assumptive and
                gratuitous.

                    CamperBob2: Username checks out, at least.

                        salawat: Logic needs to be shown the door on occasion. Sometimes via the help of
                        an ole Irish bar toss.

                        AdieuToLogic: > Username checks out, at least. Thoughtful retorts such as this
                        are deserving of the same esteem one affords the "rubber v glue"[0] idiom. As
                        such, I must oblige. 0 -
                        https://idioms.thefreedictionary.com/I%27m+rubber%2c+you%27r...

                lkey: What stole the joy you must have felt, fleetingly, as a child that beheld
                the world with fresh eyes, full of wonder? Did you imagine yourself then, as
                your are now, hunched over a glowing rectangle. Demanding imperiously that the
                world share your contempt for the sublime.  Share your jaundiced view of those
                that pour the whole of themselves into the act of creation, so that everyone
                might once again be graced with wonder anew. I hope you can find a work of art
                that breaks you free of your resentment.

                    kuerbel: Love your comment. I took the liberty of pasting it to chatgpt and
                    asked it to write another paragraph in the same style: Perhaps it is easier to
                    sneer than to feel, to dull the edges of awe before it dares to wound you with
                    longing. Cynicism is a tidy shelter: no drafts of hope, no risk of being moved.
                    But it is also a small room, airless, where nothing grows. Somewhere beyond that
                    glowing rectangle, the world is still doing its reckless, generous thing—colors
                    insisting on being seen, sounds reaching out without permission, hands shaping
                    meaning out of nothing. You could meet it again, if you chose, not as a judge
                    but as a witness, and remember that wonder is not naïveté. It is courage,
                    practiced quietly.

                        exodust: Plot twist. The comment you love is the cynical one, responding to
                        someone who clearly embraces the new by rising above caution and concern. Your
                        GPT addition has missed the context, but at least you've provided a nice little
                        paradox.

                        balamatom: Thank you for the AI warning, so I didn't have to read that.

                    ceuk: Thank you for brightening my morning with a brief moment of romantic
                    idealism in a black ocean of cynicism

        sonofhans: Ironic. The frequency and predictability of this type of response —
        “This criticism of new technology is invalid because someone was wrong once in
        the past about unrelated technology” — means there might as well be an LLM
        posting these replies to every applicable article. It’s boring and no one learns
        anything. It would be a lot more interesting to point out the differences and
        similarities yourself. But then if you wanted an interesting discussion you
        wouldn’t be posting trite flamebait in the first place, would you?

        dwrolvink: Interesting comparison. I remember watching a video on that.
        Landscape paintings, portraits, etc, was an art that has taken an enormous
        nosedive. We, as humans, have missed out on a lot of art because of the
        invention of the camera. On the other hand, the benefits of the camera need no
        elaboration. Currently AI had a lot of foot guns though, which I don't believe
        the camera had. I hope AI gets to that point too.

            jack_pp: The footgun cameras had was exposure time. 1826 - The Heliograph - 8+
            hours 1839 - The Daguerreotype - 15–30 Mins 1841 - The Calotype - 1–2 Mins 1851
            - Wet Plate Collodion - 2–20 Secs 1871 - The Dry Plate - < 1 Second. So it took
            45 years to perfect the process so you could take an instant image. Yet we
            complain after 4 years of LLMs that they're not good enough.

        cjohnson318: Yeah, and cameras changed art forever.

            exodust: people still make clay pots and paint landscapes

                navigate8310: Creativity is not what would expect out of the Renaissance

    abhgh: This is an amazing quote - thank you. This is also my argument for why I
    can't use LLMs for writing (proofreading is OK) - what I write is not produced
    as a side-effect of thinking through a problem, writing is how I think through a
    problem.

        Cthulhu_: Counterpoint (more devil's advocate), I'd argue it's better than an
        LLM writes something (e.g. the solution or thinking through of a problem) than
        nothing at all. Counterpoint to my own counterpoint, will anyone actually (want
        to) read it? counterpoint to the third degree, to loop it back around, an LLM
        might and I'd even argue an LLM is better at reading and ingesting long text
        (I'm thinking architectural documentation etc) than humans are. Speaking for
        myself, I struggle to read attentively through e.g. a document, I quickly lose
        interest and scan read or just focus on what I need instead.

        samusiam: Writing is how I think through a problem too, but that also applies to
        writing and communicating with an AI coding agent. I don't need to write the
        code per se to do the thinking.

    oceanplexian: Coding is not at all like working a lump of clay unless you’re
    still writing assembly. You’re taking a bunch of pre-built abstractions written
    by other people on top of what the computer is actually doing and plugging them
    together like LEGOs. The artificial syntax that you use to move the bricks
    around is the thing you call coding. The human element of discovery is still
    there if a robot stacks the bricks based on a different set of syntax (Natural
    Language), nothing about that precludes authenticity or the human element of
    creation.

        satvikpendem: Exactly, and that's why I find AI coding solving this well,
        because I find it tedious to put the bricks together for the umpteenth time when
        I can just have an AI do it (which I will of course verify the code when it's
        done, not advocating for vibe coding here). This actually leaves me with a lot
        more time to think, about what I want the UI to look like, how I'll market my
        software, and so on.

        lsy: I think the analogy to high level programming languages misunderstands the
        value of abstraction and notation. You can’t reason about the behavior of an
        English prompt because English is underspecified. The value of code is that it
        has a fairly strong semantic correlation to machine operations, and reasoning
        about high level code is equivalent to reasoning about machine code. That’s why
        even with all this advancement we continue to check in code to our repositories
        and leave the sloppy English in our chat history.

        hnlmorg: You’re both right. It just depends on the problems you’re solving and
        the languages you use. I find languages like JavaScript promote the idea that of
        “Lego programming” because you’re encouraged to use a module for everything. But
        when you start exploring ideas that haven’t been thoroughly explored already,
        and particularly in systems languages which are less zealous about DRY (don’t
        repeat yourself) methodologies, the you can feel a lot more like a sculptor.
        Likewise if you’re building frameworks rather than reusing them. So it really
        depends on the problems you’re solving. For general day-to-day coding for your
        average 9-to-5 software engineering job, I can definitely relate to why people
        might think coding is basically “LEGO engineering”.

        Jensson: > Coding is not at all like working a lump of clay unless you’re still
        writing assembly. Isn't the analogy apt? You can't make a working car using a
        lump of clay, just a car statue, a lump of clay is already an abstraction of
        objects you can make in reality.

            balamatom: Bingo.

        vaylian: > You’re taking a bunch of pre-built abstractions written by other
        people on top of what the computer is actually doing and plugging them together
        like LEGOs. Correct. However, you will probably notice that your solution to the
        problem doesn't feel right, when the bricks that are available to you, don't
        compose well. The AI will just happily smash together bricks and at first glance
        it might seem that the task is done. Choosing the right abstraction (bricks) is
        part of finding the right solution. And understanding that choice often requires
        exploration and contemplation. AI can't give you that.

            Cthulhu_: Not yet, anyway; I do trust LLMs for writing snippets or features at
            this point, but I don't trust them for setting up new applications, technology
            choices, architectures, etc. The other day people were talking about metrics,
            the amount of lines of code people vs LLMs could output in any given time, or
            the lines of code in an LLM assisted application - using LOC as a metric for
            productivity. But would an LLM ever suggest using a utility or library, or re-
            architecture an application, over writing their own code? I've got a fairly
            simple application, renders a table (and in future some charts) with metrics. At
            the moment all that is done "by hand", last features were stuff like filtering
            and sorting the data. But that kind of thing can also be done by a "data table"
            library. Or the whole application can be thrown out in favor of a workbook (one
            of those data analysis tools, I'm not at home in that are at all). That'd save
            hundreds of lines of code + maintenance burden.

                z3dd: I was creating a Jira/bb wrapper with node recently and Claude actually
                used plenty of libraries to solve some tasks.

        hennell: It depends what you're doing not really what you do it with. I can do
        some crud apps where it's just data input to data store to output with little
        shaping needed. Or I can do apps where there's lots of filters, actions and
        logic to happen based on what's inputted that require some thought to ensure
        actually solve the problem it's proposed for. "Shaping the clay" isn't about the
        clay, it's about the shaping. If you have to make a ball of clay and also have
        to make a bridge of Lego a 175kg human can stand on, you'll learn more about
        Lego and building it than you will about clay. Get someone to give you a Lego
        instruction sheet and you'll learn far less, because you're not shaping anymore.

    isolli: This is very insightful, thanks. I had a similar thought regarding data
    science in particular. Writing those pandas expressions by hand during
    exploration means you get to know the data intimately. Getting AI to write them
    for you limits you to a superficial knowledge of said data (at least in my
    case).

    nielsbot: While there is still a market for artisanal furniture, dishes and
    clothes most people buy mass-produced dishes, clothes and furniture. I wonder if
    software creation will be in a similar place. There still might be a small
    market for handmade software but the majority of it will be mass produced. (That
    is, by LLM or even software itself will mostly go away and people will get their
    work done via LLM instead of "apps")

        pinkgolem: I would argue the opposite.. What you get right now is mass
        replicated software, just another copy of sap/office/Spotify/whatever That
        software is not made individually for you, you get a copy like millions of other
        people and there is nearly no market anymore for individual software. Llms might
        change that, we have a bunch of internal apps now for small annoying things..
        They all have there quirks, but are only accessible internally and make life a
        little bit easier for people working for us. Most of them are one shot llms
        things, throw away if you do not need it anymore or just one shoot again

            Cthulhu_: The question is whether that's a good thing or not; software adages
            like "Not Invented Here" aren't going to go away. For personal tools /
            experiments it's probably fine, just like hacking together something in your
            spare time, but it can become a risk if you, others, or a business start to
            depend on it (just like spare time hacked tools). I'd argue that in most cases
            it's better to do some research and find out if a tool already exists, and if it
            isn't exactly how you want it... to get used to it, like one did with all other
            tools they used.

        Cthulhu_: As with furniture, it's supply vs demand, and it's a discussion that
        goes back decades at this point. Very few people (even before LLM coding tools)
        actually did low level "artisanal" coding; I'd argue the vast majority of
        software development goes into implementing features in b2b / b2c software,
        building screens, logins, overviews, detail pages, etc. That requires
        (required?) software engineers too, and skill / experience / etc, but it was
        more assembling existing parts and connecting them. Years ago there was already
        a feeling that a lot of software development boiled down to taping libraries
        together. Or from another perspective, replace "LLM" with "outsourcing".

        intended: Acceptance of mass production is only post establishment of quality
        control. Skipping over that step results in a world of knock offs and product
        failures. People buy Zara or H&M because they can offload the work of verifying
        quality to the brand. This was a major hurdle that mass manufacturing had to
        overcome to achieve dominance.

    socalgal2: To me it's all abstraction. I didn't write my own OS. I didn't write
    my own compiler. I didn't write the standard library. I just use them. I could
    write them but I'm happy to work on the new thing that uses what's already
    there. This is no different than many things. I could grow a tree and cut it
    into wood but I don't. I could buy wood and nails and brackets and make
    furniture but I don't. I instead just fill my house/apartment with stuff already
    made and still feel like it's mine. I made it. I decided what's in it. I didn't
    have to make it all from scratch. For me, lots of programming is the same. I
    just want to assemble the pieces > When you skip the process of creation you
    trade the thing you could have learned to make for the simulacrum of the thing
    you thought you wanted to make No, your favorite movie is not crap because the
    creators didn't grind their own lens. Popular and highly acclaimed games not at
    crap because they didn't write their own physics engine (Zelda uses Havok) or
    their own game engine (Plenty of great games use Unreal or Unity)

        Krssst: OS and compilers have a deterministic public interface. They obey a
        specification developers know, so you they can be relied on to write correct
        software that depends on them even without knowing the internal behavior.
        Generative AI does not have those properties.

            refactor_master: But the code you’re writing is guard railed by your oversight,
            the tests you decide on and the type checking. So whether you’re writing the
            spec code out by hand or ask an LLM to do it is besides the point if the code is
            considered a means to an end, which is what the post above yours was getting at.

        globular-toast: There are two stages to becoming a decent programmer: first you
        learn to use abstraction, then you learn when not to use abstraction. Trying to
        find the right level is the art. Once you learn the tools of the trade and can
        do abstraction, it's natural to want to abstract everything. Most programmers go
        through such a phase. But sometimes things really are distinct and trying to
        find an abstraction that does both will never be satisfactory. When building a
        house there are generally a few distinct trades that do the work: bricklayers,
        joiners, plumbers, electricians etc. You could try to abstract them all: it's
        all just joining stuff together isn't it? But something would be lost. The
        dangers of working with electricity are completely different to working with
        bricks. On the other hand, if people were too specialised it wouldn't work
        either. You wouldn't expect a whole gang of electricians, one who can only do
        lighting, one who can only do sockets, one who can only do wiring etc. After
        centuries of experience we've found a few trades that work well together. So,
        yes, it's all just abstraction, but you can go too far.

            throwaway132448: Well said, great analogy. Sometimes the level of abstraction
            feels arbitrary - you have to understand the circumstances that led there to see
            why it's not.

        tonyedgecombe: >I instead just fill my house/apartment with stuff already made
        and still feel like it's mine. I'm starting to wonder if we lose something in
        all this convenience. Perhaps my life is better because I cook my own food, wash
        my own dishes, chop my own firewood, drive my own car, write my own software.
        Outwardly the results look better the more I outsource but inwardly I'm not so
        sure. On the subject of furnishing your house the IKEA effect seems to confirm
        this. https://en.wikipedia.org/wiki/IKEA_effect

        yason: The creative process is not dependent on the abstraction. > For me, lots
        of programming is the same. I just want to assemble the pieces How did those
        pieces came to be? By someone assembling other pieces or by someone crafting
        them together out of nowhere because nobody else had written them by the time?
        Of course you reuse other parts and abstractions to do whatever things that
        you're not working on but each time you do something that hasn't been done
        before you can't but engage the creative process, even if you're sitting on top
        of 50 years worth of abstractions. In other words, what a programmer essentially
        has is a playfield. And whether the playfield is a stack of transistors or
        coding agents, when you program you create something new even if it's defined
        and built in terms of the playfield.

        jstanley: > I didn't write my own OS. I didn't write my own compiler. I didn't
        write the standard library. I just use them. I could write them Maybe, but
        beware assuming you could do something you haven't actually tried to do.
        Everything is easy in the abstract.

        Hendrikto: > No, your favorite movie is not crap because the creators didn't
        grind their own lens. But Pulp Fiction would not have been a masterpiece if
        Tarantino just typed “Write a gangster movie.” into a prompt field.

    bodge5000: "The muse visits during the act of creation, not before. Start
    alone." That has actually been a major problem for me in the past where my core
    idea is too simple, and I don't give "the muse" enough time to visit because it
    doesn't take me long enough to build it. Anytime I have given the muse time to
    visit, they always have.

    ibestvina: This makes no sense to me. There are plenty of artists out there
    (e.g. El Anatsui), not to mention whole professions such as architects, who do
    not interact directly with what they are building, and yet can have profound
    relationship with the final product. Discovering the right problem to solve is
    not necessarily coupled to being "hands on" with the "materials you're shaping".

        darepublic: you think  El Anatsui would concur that they didn't interact
        directly with what they were building?  "hands on", "material you're shaping" is
        a metaphor

            ibestvina: I don't see why his involvement, explaining to his team how exactly
            to build a piece, is any different from a developer explaining to an LLM how to
            build a certain feature, when it comes to the level of "being hands on".
            Obviously I am not comparing his final product with my code, I am simply
            pointing out how this metaphor is flawed. Having "workers" shape the material
            according to your plans does not reduce your agency.

        lolive: In my company, [enterprise IT] architects are separated into two kinds.
        People with a CV longer than my arm who know/anticipate everything that could
        fail and have reached a level of understandind that I personnally call "wisdom".
        And theorists, who read books and norms, who focus mostly on the nominal case,
        and have no idea [and no interest] in how the real world will be a hard brick
        wall that challenges each and every idea you invent. Not being hands-on, and
        more important not LISTENING to the hands-on people and learning from them, is a
        massive issue in my surroundings. So thinking hard on something is cool. But
        making it real is a whole different story. Note: as Steve used to say, "real
        artists ship".

    darepublic: Thanks for the quote, it definitely resonates.  Distressing to see
    many people who can't relate to this, taking it literally and arguing that there
    is nothing lost the more removed they are from the process.

    Bengalilol: I love Aral, he is so invested.

    Cuervo_: I personally have found success with an approach that's the inverse of
    how agents are being used generally. I don't allow my agent to write any code. I
    ask it for guidance on algorithms, and to supply the domain knowledge that I
    might be missing. When using it for game dev for example, I ask it to explain in
    general terms how to apply noise algorithms for procedural generation, how to do
    UV mapping etc, but the actual implementation in my language of choice is all by
    hand. Honestly, I think this is a sweet spot. The amount of time I save getting
    explanations of concepts that would otherwise get a bit of digging to get is
    huge, but I'm still entirely in control of my codebase.

    CraigJPerry: >> Coding is like That description is NOT coding, coding is a
    subset of that. Coding comes once you know what you need to build, coding is the
    process of you expressing that in a programming language and as you do so you
    apply all your knowledge, experience and crucially your taste, to arrive at an
    implementation which does what's required (functionally and non-functionally)
    AND is open to the possibility of change in future. Someone else here wrote a
    great comment about this the other day and it was along the lines of if you take
    that week of work described in the GP's comment, and on the friday afternoon you
    delete all the code checked in. Coding is the part to recreate the check in,
    which would take a lot less than a week! All the other time was spent turning
    you into the developer who could understand why to write that code in the first
    place. These tools do not allow you to skip the process of creation. They allow
    you to skip aspects of coding - if you choose to, they can also elide your
    tastes but that's not a requirement of using them, they do respond well to
    examples of code and other directions to guide them in your tastes. The
    functional and non-functional parts they're pretty good at without much steering
    now but i always steer for my tastes because, e.g. opus 4.5 defaults to a more
    verbose style than i care for.

        pikzel: It's all individual. That's like saying writing only happens when you
        know exactly the story to tell. I love open a blank project with a vague idea of
        what I want to do, and then just start exploring while I'm coding.

    moron4hire: I have no idea who this guy is (I guess he's a fantasy novelist?)
    but this video came up in my YouTube feed recently and feels like it matches
    closely with the themes you're expressing. https://youtu.be/mb3uK-
    _QkOo?si=FK9YnawwxHLdfATv

    jstanley: But you can move a layer up. Instead of pouring all of your efforts
    into making one single static object with no moving parts, you can simply
    specify the individual parts, have the machine make them for you, and pour your
    heart and soul into making a machine that is composed of thousands of parts,
    that you could never hope to make if you had to craft each one by hand from
    clay. We used to have a way to do this before LLMs, of course: we had companies
    that employed many people, so that the top level of the company could simply
    specify what they wanted, and the lower levels only had to focus on making
    individual parts. Even the person making an object from clay is (probably) not
    refining his own clay or making his own oven.

        amelius: Yes, but bad ingredients do not make a yummy pudding. Or, it's like
        trying to make a MacBook Pro by buying electronics boards from AliExpress and
        wiring them together.

            jstanley: I'd rather have a laptop made from AliExpress components than only
            have a single artisanal hand-crafted resistor.

    leftbehinds: reminds of arguments for - hosting a server vs running stuff in
    cloud - vps vs containers

tbmtbmtbmtbmtbm: Make sure you start every day with the type of confidence that
would allow you to refer to yourself as an intellectual one-percenter

tayo42: >  I tried getting back in touch with physics, reading old textbooks.
But that wasn’t successful either. It is hard to justify spending time and
mental effort solving physics problems that aren’t relevant or state-of-the-art
I tried this with physics and philosophy. I think i want to do a mix of hard but
meaningful. For academic fields like that its impossible for a regular person to
do as a hobby. Might as well just do puzzles or something.

themafia: > Yes, I blame AI for this. Just don't use it.  That's always an
option.  Perhaps your builder doesn't actually benefit from an unlimited runway
detached from the cost of effort.

bariswheel: Good highlight of the struggle between Builder and Thinker, I
enjoyed the writing. So why not work on PQC? Surely you've thought about other
avenues here as well. If you're looking for a domain where the 70% AI solution
is a total failure, that's the field. You can't rely on vibe coding because the
underlying math, like Learning With Errors (LWE) or supersingular isogeny
graphs, is conceptually dense and hasn't been commoditized into AI training data
yet. It requires that same 'several-day-soak' thinking you loved in physics,
specifically because we're trying to build systems that remain secure even
against an adversary with a quantum computer. It’s one of the few areas left
where the Thinker isn't just a luxury, but a hard requirement for the Builder to
even begin.

Der_Einzige: Instant upvote for a Philiip Mainlander quote at the end. He's the
OG "God is Dead" guy and Nietzsche was reacting (very poorly) to Mainlander and
other pessimists like Schopenhauer when he followed up with his own, shittier
version of "god is dead" Please read up on his life. Mainlander is the most
extreme/radical Philosophical Pessimist of them all. He wrote a whole book about
how you should rationally kill yourself and then he killed himself shortly
after. https://en.wikipedia.org/wiki/Philipp_Mainl%C3%A4nder
https://dokumen.pub/the-philosophy-of-redemption-die-philoso... Max Stirner and
Mainlander would have been friends and are kindred spirits philosophically.
https://en.wikipedia.org/wiki/Bibliography_of_philosophical_...

sfink: I definitely relate to this. Except that while I was in the 1% in
university who thought hard, I don't think my success rate was that high. My
confidence in the time was quite high, though, and I still remember the notable
successes. And also, I haven't started using AI for writing code yet. I'm
shuffling toward that, with much trepidation. I ask it lots of coding questions.
I make it teach me stuff. Which brings me to the point of my post: The other
day, I was looking at some Rust code and trying to work out the ownership rules.
In theory, I more or less understand them. In practice, not so much. So I had
Claude start quizzing me. Claude was a pretty brutal teacher -- he'd ask 4 or 5
questions, most of them solvable from what I knew already, and then 1 or 2 that
introduced a new concept that I hadn't seen. I would get that one wrong and ask
for another quiz. Same thing: 4 or 5 questions, using what I knew plus the thing
just introduced, plus 1 or 2 with a new wrinkle. I don't think I got 100% on any
of the quizzes. Maybe the last one; I should dig up that chat and see. But I
learned a ton, and had to think really hard. Somehow, I doubt this technique
will be popular. But my experience with it was very good. I recommend it. (It
does make me a little nervous that whenever I work with Claude on things that
I'm more familiar with, he's always a little off base on some part of it. Since
this was stuff I didn't know, he could have been feeding me slop. But I don't
think so; the explanations made sense and the the compiler agreed, so it'd be
tough to get anything completely wrong. And I was thinking through all of it;
usually the bullshit slips in stealthily in the parts that don't seem to matter,
but I had to work through everything.)

Animats: "Sometimes you have to keep thinking past the point where it starts to
hurt." - Fermi

anonymous344: yes but you solved problems already solved by someone else. how
about something that hasn't been solved, or yet even noticed? that gives the
greatest satisfaction

rc-1140: I think what plagues a lot of pure STEM types in this tumultuous period
of AI (or "AI") is that they've spent a majority of their lives mulling over
some problem until they've worked out every possible imperfection, and once
they've achieved something they consider close to that level of perfection,
that's when they say they're done. While this may be an unfair generalization,
and apologies to those who don't feel this way, but I believe STEM types like
the OP are used to problem solving that's linear in the sense that the problem
only exists in its field as something to be solved, and once they figure it out,
they're done. The OP even described his mentality as that of a "Thinker" where
he received a problem during his schooling, mulled over it for a long time, and
eventually came to the answer. That's it, next problem to crack. Their whole
lives revolve around this process and most have never considered anything
outside it. Even now, despite my own healthy skepticism of and distaste for AI,
I am forced to respect that AI can do some things very fast. People like the OP,
used to chiseling away at a problem for days, weeks, months, etc., now have that
throughput time slashed. They're used to the notion of thinking long and hard
about a very specific problem and finally having some output; now, code modules
that are "good enough" can be cooked up in a few minutes, and if the module
works the problem is solved and they need to find the next problem. I think this
is more common than most people want to admit, going back to grumblings of
"gluing libraries together" being unsatisfying. The only suggestion I have for
the OP is to expand what you think about. There are other comments in this
thread supporting it but I think a sea change that AI is starting to bring for
software folks is that we get to put more time towards enhancing module design,
user experience, resolving tech debt, and so on. People being the ones writing
code is still very important. I think there's more to talk about where I do
share the OP's yearning and fears (i.e., people who weren't voracious readers or
English/literary majors being oneshot by the devil that is AI summaries, AI-
assisted reading, etc.) but that's another story for another time.

    ai_critic: > I think what plagues a lot of pure STEM types in this tumultuous
    period of AI (or "AI") is that they've spent a majority of their lives mulling
    over some problem until they've worked out every possible imperfection, and once
    they've achieved something they consider close to that level of perfection,
    that's when they say they're done. These people are miserable to work with if
    you need things done quickly and can tolerate even slight imperfection. That
    operating regime is, incidentally, 95% of the work we actually get paid to do.

foxmoss: Eventually I always get to a problem I can't solve by just throwing an
LLM at it and have to go in and properly debug things. At that point knowing the
code base helps a hell of a lot, and I would've been better off writing the
entire thing by hand.

koakuma-chan: What a bizarre claim. If you can solve anything by thinking, why
don't you become a scientist? Think of a theory that unites quantum physics and
general relativity.

keyle: I don't get it. I think just as hard, I type less. I specify precisely
and I review. If anything, all we've changed is working at a higher level. The
product is the same. But these people just keep mixing things up like "wow I got
a ferrari now, watch it fly off the road!" Yeah so you got a tools upgrade; it's
faster, it's more powerful. Keep it on the road or give up driving! We went from
auto completing keywords, to auto completing symbols, to auto completing
statements, to auto completing paragraphs, to auto completing entire features.
Because it happened so fast, people feel the need to rename programming every
week. We either vibe coders now, or agentic coders or ... or just programmers
hey. You know why? I write in C, I get machine code, I didn't write the machine
code! It was all an abstraction! Oh but it's not the same you say, it changes
every time you ask. Yes, for now, it's still wonky and janky in places. It's
just a stepping stone. Just chill, it's programming. The tools just got even
better. You can still jump on a camel and cross the desert in 3 days. Have at
it, you risk dying, but enjoy. Or you can just rent a helicopter and fly over
the damn thing in a few hours. Your choice. Don't let people tell you it isn't
travelling. We're all Linus Torvalds now. We review, we merge, we send back. And
if you had no idea what you were doing before, you'll still have no idea what
you're doing today. You just fat-finger less typos today than ever before.

    joseangel_sc: except the thing does not work as expected and it just makes you
    worse not better

        keyle: Like I said that's temporary. It's janky and wonky but it's a stepping
        stone. Just look at image generation. Actually factually look at it. We went
        from horror colours vomit with eyes all over, to 6 fingers humans, to pretty
        darn good now. It's only time.

            leecommamichael: Why is image generation the same as code generation?

                dcw303: it's not. We were able to get rid of 6 fingered hands by getting very
                specific, and fine tuning models with lots of hand and finger training data. But
                that approach doesn't work with code, or with reasoning in general, because you
                would need to exponentially fine tune everything in the universe. The illusion
                that the AI "understands" what it is doing is lost.

                rvz: It isn't. Code generation progression in LLMs still carries higher
                objective risk of failure depending on the experience on the person using it
                because: 1. They still do not trust if the code works (even if it has tests)
                thus, needs thorough human supervision and still requires on-going maintainance.
                2. Hence (2) it can cost you more money than the tokens you spent building it in
                the first place when it goes horribly wrong in production. Image generation
                progression comes with close to no operational impact, and has far less human
                supervision and can be safely done with none.

        beebmam: Comments like these are why I don't browse HN nearly ever anymore

            w4yai: Nothing new. Whenever a new layer of abstraction is added, people say
            it's worse and will never be as good as the old way. Though it's a totally
            biased opinion, we just have issues with giving up things we like as human
            being.

                roadbuster: > Whenever a new layer of abstraction is added LLMs aren't a "layer
                of abstraction." 99% of people writing in assembly don't have to drop down into
                manual cobbling of machine code. People who write in C rarely drop into
                assembly. Java developers typically treat the JVM as "the computer." In the OSI
                network stack, developers writing at level 7 (application layer) almost never
                drop to level 5 (session layer), and virtually no one even bothers to understand
                the magic at layers 1 & 2. These all represent successful, effective
                abstractions for developers. In contrast, unless you believe 99% of "software
                development" is about to be replaced with "vibe coding", it's off the mark to
                describe LLMs as a new layer of abstraction.

                    w4yai: > unless you believe 99% of "software development" is about to be
                    replaced with "vibe coding" Probably not vibe coding, but most certainly with
                    some AI automation

                duskdozer: The difference is that LLM output is very nondeterministic.

                    w4yai: It depends. Temperature is a variable. If you really need determinism,
                    you could build a LLM for that. Non-determinism can be a good feature though.

        CrimsonRain: That's your opinion and you can not use those tools. People are
        paying for it because it helps them. Who are you to whine about it?

            nunez: But that's the entire flippin' problem. People are being forced to use
            these tools professionally at a stagering rate. It's like the industry is in its
            "training your replacement" era.

                CrimsonRain: you don't like it? Find a place that doesn't enforce it. Can't find
                it? Then either build it or accept that you want a horse carriage while people
                want taxi.

    tired-turtle: > We're all Linus Torvalds now. So...where's your OS and SCM? I
    get your point that wetware stills matter, but I think it's a bit much to
    contend that more than a handful of people (or everyone) is on the level of
    Linus Torvalds now that we have LLMs.

        keyle: I should have been clearer. It was a pun, a take, a joke. I was referring
        to his day-to-day activity now, where he merges code, doesn't write hardly any
        code for the linux kernel. I didn't imply most of use can do half the thing he's
        done. That's not right.

            tired-turtle: > his day-to-day activity now, where he merges code But even
            then...don't you think his insight into and ability to verify a PR far exceeds
            that of most devs (LLM or not)? Most of us cannot (reasonably) aspire to be like
            him.

                keyle: Like I said if you didn't know what you were doing before, you won't know
                what you're doing with today. Agentic coding in general only amplify your
                ability (or disability). You can totally learn how to build an OS and invest 5
                years of your life doing so. The first version of Linux I'm sure was pretty
                shoddy. Same for a SCM. I've been doing this for 30 years. At some point, your
                limit becomes how much time you're willing to invest in something.

                DANmode: But some can aspire to be him circa five years ago, while Linus has his
                own efforts multiplied as well.

        fragmede: My hair hasn't turned blonde and I don't suddenly know how to speak
        Finnish, either. You might have missed their point.

    hnfong: I think I understand what the author is trying to say. We miss thinking
    "hard" about the small details. Maybe "hard" isn't the right adjective, but we
    all know the process of coding isn't just typing stuff while the mind wanders.
    We keep thinking about the code we're typing and the interactions between the
    new code and the existing stuff, and keep thinking about potential bugs and
    issues. (This may or may not be "hard".) And this kind of thinking is totally
    different from what Linus Torvalds has to think about when reviewing a huge
    patch from a fellow maintainer. Linus' work is probably "harder", but it's a
    different kind of thinking. You're totally right it's just tools improving. When
    compilers improved most people were happy, but some people who loved hand
    crafting asm kept doing it as a hobby. But in 99+% cases hand crafting asm is a
    detriment to the project even if it's fun, so if you love writing asm yourself
    you're either out of work, or you grudgingly accept that you might have to write
    Java to get paid. I think there's a place for lamenting this kind of situation.

        Helmut10001: I agree. I think some of us would rather deal with small,
        incremental problems than address the big, high-level roadmap. High-level things
        are much more uncertain than isolated things that can be unit-tested. This can
        create feelings of inconvenience and unease.

        jtrn: Spot on. It’s the lumberjack mourning the axe while holding a chainsaw.
        The work is still hard. it’s just different.  The friction comes from developers
        who prioritize the 'craft' of syntax over delivering value. It results in
        massive motivated reasoning. We see people suddenly becoming activists about
        energy usage or copyright solely to justify not using a tool they dislike. They
        will hunt for a single AI syntax error while ignoring the history of bugs caused
        by human fatigue. It's not about the tech. it's about the loss of the old way of
        working. And it's also somewhat egotistical it seems to me. I sense a pattern
        that many developers care more about doing what they want instead of providing
        value to others.

            alkonaut: I disagree. It's like the lumberjack working from home watching an
            enormous robotic forestry machine cut trees on a set of tv-screens. If he
            enjoyed producing lumber, then what he sees on those screens will fill him with
            joy. He's producing lots of lumber. He's much more efficient than with both axe
            and chainsaw. But if he enjoyed being in the forest, and _doesn't really care
            about lumber at all_ (Because it turns out, he never used or liked lumber, he
            merely produced it for his employer) then these screens won't give him any joy
            at all. That's how I feel. I don't care about code, but I also don't really care
            about products. I mostly care about the craft. It's like solving sudokus. I
            don't collect solved sudokus. Once solved I don't care about them. Having a
            robot solve sudokus for me would be completely pointless. > I sense a pattern
            that many developers care more about doing what they want instead of providing
            value to others. And you'd be 100% right. I do this work because my employer
            provides me with enough sudokus. And I provide value back which is more than I'm
            compensated with. That is: I'm compensated with two things: intellectual
            challenge, and money. That's the relationship I have with my employer. If I
            could produce 10x more but I don't get the intellectual challenge? The employer
            isn't giving me what I want - and I'd stop doing the work. I think "You do what
            the employer wants, produce what needs to be produced, and in return you get
            money" is a simplification that misses the literal forest for all the forestry.

                jstummbillig: But now you are conflating solving problems with a personal
                preference of how the problem should be solved. This never bodes well (unless
                you always prefer picking the method best suited to solve the problem.)

                    alkonaut: Well as I said, I consider myself compensated with intellectual
                    challenge/stimulus as part of my compensation. It's _why_ I do the work to begin
                    with. Or to put it another way: it's either done in a way I like, or it's
                    probably not done at all. I'm replaceable after all. If there is someone who is
                    better and more effective at solving problems in some objectively good way -
                    they should have my job. The only reason I still have it is because it seems
                    this is hard to find. Employers are stuck with people who solve problems in the
                    way they like for varying personal reasons and not the objectively best way of
                    solving problems. The hard part in keeping employees happy is that you can't
                    just throw more money at them to make them effective. Keeping them stimulated is
                    the difficult part. Some times you must accept that you must perhaps solve a
                    problem that isn't the most critical one to address, or perhaps a bad call
                    business wise, to keep employees happy, or keep them at all. I think a lot of
                    the "Big rewrites" are in this category, for example. Not really a good idea
                    compared to maintenance/improvement, but if the alternative is maintaining the
                    old one _and_ lose the staff who could do that?

            latexr: > We see people suddenly becoming activists about energy usage or
            copyright solely to justify not using a tool they dislike. Maybe you don’t care
            about the environment (which includes yourself and the people you like), or
            income inequality, or the continued consolidation of power in the hands of a few
            deranged rich people, or how your favourite artists (do you have any?) are
            exploited by the industry, but some of us have been banging the drum about those
            issues for decades. Just because you’re only noticing it now or don’t care it
            doesn’t mean it’s a new thing or that everyone else is being duplicitous. It’s a
            good thing more people are waking up and talking about those.

    tdstein: > You just fat-finger less typos today than ever before. My typos are
    largely admissible.

    rising-sky: I think the more apt analog isn't a faster car, a la Ferrari, it's
    more akin to someone who likes to drive and now has to sit and monitor the self-
    driving car steer and navigate. Comparing to the Ferrari is incorrect since it
    still takes a similar level of agency from the driver versus a <insert slower
    vehicle>

        nunez: This is exactly the right analogy here. FSD is very very good most of the
        time. It's so good (well, v14 is, anyway), it makes it easy to get lulled into
        thinking that it works all the time. So you check your watch here, check your
        phone there, and attend to other things, and it's all good until the car decides
        to turn into a curb (which almost happened to me the other day) or swerve hard
        into a tree (which happened to someone else). Funny enough, much like AI, Tesla
        is shoving FSD down people's throats by gating Autopilot 2, a lane keeping
        solution that worked extremely well and is much friendlier to people who want
        limited autonomy here and there, behind the $99/mo FSD sub (and removing the
        option to pay for the package out of pocket).

    anavat: It is simple. Continuing your metaphor, I have a choice of getting
    exactly where I want on a camel in 3 days, or getting to a random location
    somewhere on the other side of the desert on a helicopter in few hours. And
    being a reasonable person I, just like the author, choose the helicopter. That's
    it, that's the whole problem.

        augment_me: You did something smart and efficinently using the least amount of
        energy and time needed. +1 for consciousness being a mistake

        satvikpendem: Why is that the reasonable choice if it doesn't get you to your
        destination? I too did a lot of AI coding but when I saw the spaghetti it made,
        I went back to regular coding, with ask mode not agent mode as a search engine.

            anavat: Because of compound efficiency and technological enablement. Or, risking
            to beat the metaphor to death, because over a span of time I'll cross many more
            deserts than I would have on a camel, and because I'll cross deserts that I
            wouldn't even try crossing on a camel.

                satvikpendem: Why does it matter how many deserts you cross if you never get to
                where you want to go? I similarly can take 10 flights across oceans but never
                end up in the city I'm trying to visit. Sounds like in your metaphor the person
                is just crossing desserts because they want to with no goal or destination in
                mind.

    globular-toast: I get it. I got excited about agents because I told myself it
    would be "just faster typing". I told myself that my value was never as a typist
    and that this is just the latest tool like all the tools I had eagerly added to
    my kit before. But the reality is different. It's not just typing for me. It's
    coming up with crap. Filling in the blanks. Guessing. The huge problem with all
    these tools is they don't know what they know and what they don't. So when they
    don't know they just guess. It's absolutely infuriating. It's not like a
    Ferrari. A Ferrari does exactly what I tell it to, up to the first-order effects
    of how open the throttle is, what direction the wheels face, how much pressure
    is on the brakes etc. The second-order effects are on me, though. I have to
    understand what effect these pressures will have on my ultimate position on the
    road. A normie car doesn't give you as much control but it's less likely to come
    off the road. Agents are like a teleport. You describe where you want to be and
    it just takes you directly there. You say "warm and sunny" and you might get to
    the Bahamas, but you might also get to the Sahara. So you correct: "oh no, I
    meant somewhere nice" and maybe you get to the Bahamas. But because you didn't
    travel there yourself you failed to realise what you actually got. Yeah, it's
    warm, sunny and nice, but now you're on an island in the middle of nowhere and
    have to import basically everything. So I prompt again and rewrite the entire
    codebase, right? Linus Torvalds works with experts that he trusts. This is like
    a manic 5 year old that doesn't care but is eager to work. Saying we all get to
    be Torvalds is like saying we all get to experience true love because we have
    access to porn.

    nunez: You _think_ you're thinking as hard. Reading code != writing it. Just
    like watching someone do a thing isn't the same as actually doing it.

        abm53: Correct… reading code is a much more difficult and ultimately,
        productive, task. I suspect those using the tools in the best way are thinking
        harder than ever for this reason.

            dns_snek: > reading code is a much more difficult Not inherently, no. Reading it
            and getting a cursory understanding is easy, truly understanding what it does
            well, what it does poorly, what the unintended side effects might be, that's the
            difficult part. In real life I've witnessed quite a few intelligent and
            experienced people who truly believe that they're thinking "really hard" and
            putting out work that's just as good as their previous, pre-AI work, and they're
            just not. In my experience it roughly correlates to how much time they think
            they're saving, those who think they're saving the most time are in fact cutting
            corners and putting out the sloppiest quality work.

    rvz: > We're all Linus Torvalds now. We review, we merge, we send back. And if
    you had no idea what you were doing before, you'll still have no idea what
    you're doing today. You just fat-finger less typos today than ever before.
    Except Linus understands the code that is being reviewed / merged in since he
    already built the kernel and git by hand. You only see him vibe-coding toys but
    not vibe-coding in the kernel. Today, we are going to see a gradual skill
    atrophy with developers over-relying on AI and once something like Claude goes
    down, they can't do any work at all. The most accurate representation is that AI
    is going to rapidly make lots of so-called 'senior engineers' who are over-
    reliant and unable to detect bad AI code like juniors and interns.

        keyle: If you can't rebuke code today. You can't rebuke code tomorrow.

    darepublic: >You can still jump on a camel and cross the desert in 3 days. Have
    at it, you risk dying, but enjoy. Or you can just rent a helicopter and fly over
    the damn thing in a few hours. Your choice. Don't let people tell you it isn't
    travelling. its obviously not wrong to fly over the desert in a helicopter.  its
    a means to an end and can be completely preferable.  I mean myself I'd prefer to
    be in a passenger jet even higher above it, at a further remove personally.  But
    I wouldn't think that doing so makes me someone who knows the desert the same
    way as someone who has crossed it on foot.  It is okay to prefer and utilize the
    power of "the next abstraction", but I think its rather pig headed to deny that
    nothing of value is lost to people who are mourning the passing of what they
    gained from intimate contact with the territory. and no it's not just about the
    literal typing.  the advent of LLMs is not the 'end of typing', that is more
    reductionist failure to see the point.

ccortes: People here seem to be conflating thinking hard and thinking a lot.
Most examples mentioned of “thinking hard” in the comments sound like they think
about a lot of stuff superficially instead one particular problem deeply, which
is what OP is referring to.

    ghuun: If you actually have a problem worth thinking deeply about, AI usually
    can’t help with it. For example, AI can’t help you make performant stencil
    buffers on a Nokia Ngage for fun. It just doesn’t have that in it. Plenty of
    such problems abound, especially in domains involving some or the other extreme
    (like high throughput traffic). Just the other day someone posted a vibe coded
    Wikipedia project that took ages to load (despite being “just” 66MB) and
    insisted it was the best it was possible to do, whereas Google can load the
    entire planet (perceptually) in a fraction of a second.

harrisonjackson: I believe it is a type of burnout. AI might have accelerated
both the work and that feeling. I found that doing more physical projects helped
me. Large woodworking, home improvement, projects. Built-in bookshelves, a huge
butcher block bar top (with 24+ hours of mindlessly sanding), rolling
workbenches, and lots of cabinets. Learning and trying to master a new skill,
using new design software, filling the garage with tools...

armchairhacker: Personally: technical problems I usually think for a couple days
at most before I need to start implementing to make progress. But I have
background things like future plans, politics, philosophy, and stories, so I
always have something to think about. Close-up technical thinking is great, but
sometimes step back and look at the bigger picture? I don't think AI has
affected my thinking much, but that's because I probably don't know how to use
it well. Whenever AI writes a lot of code, I end up having to understand if not
change most of it; either because I don't trust the AI, I have to change the
specification (and either it's a small change or I don't trust the AI to
rewrite), the code has a leaky abstraction, the specification was wrong, the
code has a bug, the code looks like it has a bug (but the problem ends up
somewhere else), I'm looking for a bug, etc. Although more and more often the AI
saves time and thinking vs. if I wrote the implementation myself, it doesn't
prevent me from having to think about the code at all and treating it like a
black box, due to the above.

theworstname: If it's this easy to convince you to stop being creative, to stop
putting in effort to think critically, then you don't deserve the fulfilment
that creativity and critical thinking can give you. These vibe coding self pity
articles are so bizarre.

ars: I think hard all the time, AI can only solve problems for me that don't
require thinking hard. Give it anything more complex and it's useless. I use AI
for the easy stuff.

phamilton: I think harder because of AI. I have to think more rigorously. I have
to find ways to tie up loose ends, to verify the result efficiently, to create
efficient feedback loops and define categorical success criteria. I've thought
harder about problems this last year than I have in a long time.

tehjoker: Why not find a subfield that is more difficult and requires some
specialization then?

sublinear: > I have tried to get that feeling of mental growth outside of coding
A few years before this wave of AI hit, I got promoted into a tech
lead/architect role. All of my mental growth since then has been learning to
navigate office politics and getting the 10k ft view way more often. I was
already telling myself "I miss thinking hard" years before this promotion. When
I build stuff now, I do it with a much clearer purpose. I have sincerely tried
the new tools, but I'm back to just using google search if anything at all. All
I did was prove to myself the bottleneck was never writing code, but deciding
why I'm doing anything at all. If you want to think so hard you stay awake at
night, try existential dread. It's an important developmental milestone you'd
have been forced to confront anyway even 1000 years ago. My point is, you might
want to reconsider how much you blame AI.

rcvassallo83: Thinking harder than I have in a long time with AI assisted
coding. As I'm providing context I get to think about what an ideal approach
would look like and often dive into a research session to analyze pros and cons
of various solutions. I don't use agents much because it's important to see how
a component I just designed fits into the larger codebase. That experience
provides insights on what improvements I need to make and what to build next.
The time I've spent thinking about the composability, cohesiveness, and
ergonomics of the code itself have really paid off. The codebase is a joy to
work in, easy to maintain and extend. The LLMs have helped me focus my cognitive
bandwidth on the quality and architecture instead of the tedious and time
consuming parts.

woah: Just work on more ambitious projects?

pixelmelt: Would like to follow your blog, is there an rss feed?

utopiah: Pre-processed food consumer complains about not cooking anymore. /s ...
OK I guess. I mean sorry but if that's revelation to you, that by using a skill
less you hone it less, you were clearly NOT thinking hard BEFORE you started
using AI. It sure didn't help but the problem didn't start then.

ggm: A lot of productive thinking happens when asleep, in the shower, in flow
walking or cycling or rowing. It's hard to rationalise this as billable time,
but they pay for outcome even if they act like they pay for 9-5 and so if I'm
thinking why I like a particular abstraction, or see analogies to another
problem, or begin to construct dialogues with mysel(ves|f) about this, and it
happens I'm scrubbing my back (or worse) I kind of "go with the flow" so to
speak. Definitely thinking about the problem can be a lot better than actually
having to produce it.

mw888: Give the AI less responsibility but more work. Immediate inference is a
great example: if the AI can finish my lines, my `if` bodies, my struct
instantiations, type signatures, etc., it can reduce my second-by-second work
significantly while taking little of my cognitive agency. These are also tasks
the AI can succeed at rather trivially. Better completions is not as sexy, but
in pretending agents are great engineers it's an amazing feature often glossed
over. Another example is automatic test generation or early correctness
warnings. If the AI can suggest a basic test and I can add it with the push of a
button - great. The length (and thus complexity) of tests can be configured
conservatively relative to the AI of the day. Warnings can just be flags in the
editors spotting obvious mistakes. Off-by-one errors for example, which might go
unnoticed for a while, would be an achievable and valuable notice. Also,
automatic debugging and feeding the raw debugger log into an AI to parse seems
promising, but I've done little of it. ...And go from there - if a well-crafted
codebase and an advanced model using it as context can generate short functions
well, then by all means - scale that up with discretion. These problems around
the AI coding tools are not at all special - it's a classic case of taking the
new tool too far too fast.

danavar: Many people here might be in a similar situation to me, but I took an
online masters program that allowed for continuing education following
completion of the degree. This has become one of my hobbies; I can take classes
at my own expense, not worry about my grades, and just enjoy learning. I can
push myself as much as I want and since the classes are hard, just completing 1
assignment is enough to force me to "think". Just sharing my experience for
people who might be looking for ways to challenge themselves intellectually.

erelong: You were walking to your destination which was three miles away You now
have a bicycle which gets you there in a third of the time You need to find
destinations that are 3x as far away than before

phromo: I am thinking harder than ever due to vibe coding. How will markets
shift? What will be in demand? How will the consumer side adapt? How do we
position? Predicting the future is a hard problem... The thinker in me is
working relentlessly since December. At least for me the thinker loves an
existential crisis like no other.

novoreorx: To be honest, I do not quite understand the author's point. If he
believes that agentic coding or AI has negative impact on being a thinker, or
prevent him from thinking critically, he can simply stop using them. Why blame
these tools if you can stop using them, and they won't have any effect on you?
In my case, my problem was often overthinking before starting to build anything.
Vibe coding rescued me from that cycle. Just a few days ago, I used openclaw to
build and launch a complete product via a Telegram chat. Now, I can act
immediately rather than just recording an idea and potentially getting to it
"someday later" To me, that's evolutional. I am truly grateful for the
advancement of AI technology and this new era. Ultimately, it is a tool you can
choose to use or not, rather than something that prevents you from thinking
more.

    ganzsz: For me personally, the problem is my teammates. The ability or will to
    critically think, or investigate existing tools in the codebase seems to
    disappear. Too often now I have to send back a PR where something is fixed using
    novel implementations instead of the single function call using existing
    infrastructure.

saulpw: The ziphead era of coding is over.  I'll miss it too.

kamaal: To me thinking hard involved the following steps- 1. Take a pen and
paper. 2. Write down what we know. 3. Write down where we want to go. 4. Write
down our methods of moving forward. 5. Make changes to 2, using 4, and see if we
are getting closer to 3. And course correct based on that. I still do it a lot.
LLM's act as assist. Not as a wholesale replacement.

rammy1234: Great article. Moment I finished reading this article, I thought of
my time in solving a UI menu problem with lot of items in it and algorithm I
came up with to solve for different screen sizes. It took solid 2 hrs of walking
and thinking. I still remember how I was excited when I had the feeling of
cracking the problem. Deep thinking is something everyone has it within and it
varies how fast you can think. But we all got it with right environment and time
we all got it in us. But thats long time ago. Now I always off load some
thinking to AI. it comes up with options and you just have to steer it. By time
it is getting better. Just ask it you know. But I feel like it is good old days
to think deep by yourself. Now I have a partner in AI to think along with me.
Great article.

drawnwren: "Before you read this post, ask yourself a question: When was the
last time you truly thought hard? ... a) All the time. b) Never. c) Somewhere in
between." What?

larodi: Well thinking hard is still there if you work on hard abstract problems.
I keep thinking very hard, even though 4 CCs pump code while I do this. Besides,
being a Gary Kasparov, playing on several tables, takes thinking.

rvz: Great, so does that mean that it is time to vibe code our own alternatives
of everything such as the Linux kernel because the AI is sure 'smarter' than all
of us? Seen a lot of DIY vibe coded solutions on this site and they are just
waiting for a security disaster. Moltbook being a notable example. That was just
the beginning.

IhateAI: I refer to it as "Think for me SaaS", and it should be avoided like the
plague. Literally, it will give your brain a disease we haven't even named yet.
It's as if I woke up in a world where half of resturaunts worldwide started
changing their name to McDonalds and gaslighting all their customers into
thinking McDonalds is better than their "from scratch" menu. Just dont use these
agentic tools, they legitimately are weapons who's target is your brain. You can
ship just as fast with autocomplete and decent workflows, and you know it. Its
weird, I dont understand why any self respecting dev would support these
companies. They are openly hostile about their plans for the software industry
(and many other verticles). I see it as a weapon being used by a sect of the
ruling class to diminsh the value of labor. While im not confident they'll be
successful, I'm very disappointed in my peers that are cheering them on in that
mission. My peers are obviously being tricked by promises of being able join
that class, but that's not what's going to happen. You're going to lose that
thinking muscle and therefor the value of your labor is going to be directly
correlated to the quantity and quality of tokens you can afford (or be given,
loaned!?) Be wary!!!

    16bitvoid: I'm with you. It scares me how quickly some of my peers' critical
    thinking and architectural understanding have noticeably atrophied over the last
    year and a half.

    7777332215: Short term thinkers versus long term thinkers. Just look at the end
    goal of these companies and you'll see why you shouldn't give them anything. To
    say it will free people of the boring tasks is so short sighted....

vasco: You don't have to miss it, buy a differential equation book and do one
per day. Play chess on hard mode. I mean there's so many ways to make yourself
think hard daily, this makes no sense. It's like saying I miss running. Get out
and run then.

conception: “We now buy our bread… it comes sliced… and sure you can just go and
make your sandwich and it won’t be a rustic, sourdough that you spent months
cultivating. Your tomatoes will be store bought not grown heirlooms. In the end…
you have lost the art of baking bread. And your sandwich making skills are lost
to time… will humanity ever bake again with these mass factories of bread? What
have we lost! Woe is me. Woe is me.”

    chadcmulligan: That is a very good analogy - sliced shop bread is tasteless and
    not that good for you compared to sourdough. Likewise awful store bought
    tomatoes taste like nothing compared to heirloom tomatoes and arguably have
    different nutritional content. Shop bread and tomatoes though can be
    manufactured without any thought of who makes them, though they can be reliably
    manufactured without someone guiding an LLM which is perhaps where the analogy
    falls down, and we always want them to be the same, but software is different in
    every form.

ares623: Rich Hickey and the Clojure folks coined the term Hammock Driven
Development. It was tongue in cheek but IMO it is an ideal to strive towards.

tolerance: I’d love to be able to see statistics that show LLM use and reception
according to certain socioeconomic factors.

    7777332215: Anything in particular you expect to see?

everyone: Guy complains about self vibe coding.. stop doing it then!! Do you
really think it's practical? Your job must be really easy if it is.

monch1962: As someone who's been coding for several decades now (i.e. I'm old),
I find the current generation of AI tools very ... freeing. As an industry,
we've been preaching the benefits of running lots of small experiments to see
what works vs what doesn't, try out different approaches to implementing
features, and so on. Pre-AI, lots of these ideas never got implemented because
they'd take too much time for no definitive benefit. You might spend hours
thinking up cool/interesting ideas, but not have the time available to try them
out. Now, I can quickly kick off a coding agent to try out any hare-brained
ideas I might come up with. The cost of doing so is very low (in terms of time
and $$$), so I get to try out far more and weirder approaches than before when
the costs were higher. If those ideas don't play out, fine, but I have a good
enough success rate with left-field ideas to make it far more justifiable than
before. Also, it makes playing around with one-person projects a lot practical.
Like most people with partner & kids, my down time is pretty precious, and tends
to come in small chunks that are largely unplannable. For example, last night I
spent 10 minutes waiting in a drive-through queue - that gave me about 8 minutes
to kick off the next chunk of my one-person project development via my phone,
review the results, then kick off the next chunk of development. Absolutely
useful to me personally, whereas last year I would've simply sat there annoyed
waiting to be serviced. I know some people have an "outsourcing Lego" type
mentality when it comes to AI coding - it's like buying a cool Lego kit, then
watching someone else assemble it for you, removing 99% of the enjoyment in the
process. I get that, but I prefer to think of it in terms of being able to
achieve orders of magnitude more in the time I have available, at close to zero
extra cost.

    marcus_holmes: Totally agree. I can spend an afternoon trying out an approach to
    a problem or product (usually while taking meetings and writing emails as well).
    If it doesn't work, then that's a useful result from my time. If it does work, I
    can then double-down on review, tests, quality, security, etc and make sure it's
    all tickety-boo.

        fuomag9: Completely agree, there’s so many small projects I’d never been able to
        even start in my free time, because I’m NOT a full-stack dev and I’d rather not
        spend all my evenings fixing or working around all the small changes and quirks
        of the $currentjsframework

    lll-o-lll: > 8 minutes to kick off the next chunk of my one-person project
    development via my phone, review the results, then kick off the next chunk of
    development. How are you doing this via your phone?

        yieldcrv: claude can deploy to github spaces and modify code for deployment to
        those by commits and pull requests to the repo exclusively claude via browser
        and claude mobile apps function this way but alongside that, people do make
        tunnels to their personal computer and setup ways to be notified on their phone,
        or to get the agent unstuck when it asks for a permission, from their phone

        fragmede: The (iOS) Claude phone app has a Claude code feature which runs "in
        the cloud". It's pretty handy for getting things done on the bus.

        samusiam: Termius + tailscale + tmux is a common setup for mobile coding
        sessions.

zkmon: When people missed working hard, they turned to fake physical work
(gyms). So people now need some fake thinking work. Except for eating and
sleeping, all other human activities are fake now.

    soanvig: you forget about fake sleeping being loaded with fake dopamine hits
    before sleep AND broken sleep schedules; and eating fake ultraprocessed food
    instead of wholefoods.

        zkmon: Thanks for correcting. That completes the fake life pattern. So, people
        fake things to get a fake life. Reminds me a Russian joke about factory workers.
        "They pretend to pay, and we pretend to work".

    rrvsh: We've always been doing fake thinking work since the beginning, see:
    puzzles

sbinnee: What OP wants to say is that they miss the process of thinking hard for
days and weeks and one day this brilliant idea popping up on their bed before
sleep. I lost my "thinking hard" process again too today at work against my
pragmatism, or more precisely my job.

6mirrors: The sampling rate we use to take input information is fixed. And we
always find a way to work with the sampled information, no matter if the input
information density is high or low. We can play a peaceful game and a intense
one. Now, when we think, we can always find a right level of abstract to think
on. Decades ago a programmer thought with machine codes, now we think with high
level concepts, maybe towards philosophy. A good outcome always requires hard
thinking. We can and we WILL think hard at a appropriate level.

urutom: One thing this discussion made me realize is that "thinking hard" might
not be a single mode of thinking. In grad school, I had what I'd call the
classic version. I stayed up all night mentally working on a topology question
about turning a 2-torus inside out. I already knew you can't flip a torus inside
out in ordinary R^3 without self-intersection. So I kept moving and stretching
the torus and the surrounding space in my head, trying to understand where the
obstruction actually lived. Sometime around sunrise, it clicked that if you
allow the move to go through infinity(so effectively S^3), the inside/outside
distinction I was relying on just collapses, and the obstruction I was
visualizing dissolves. Birds were chirping, I hadn't slept, and nothing useful
came out of it, but my internal model of space felt permanently upgraded. That's
clearly "thinking hard" in the sense. But there's another mode I've experienced
that feels related but different. With a tough Code Golf problem, I might carry
it around for a week. I'm not actively grinding on it the whole time, but the
problem stays loaded in the background. Then suddenly, in the shower or on a
walk, a compression trick or a different representation just clicks. That
doesn't feel "hard" moment to moment. It's more like keeping a problem resident
in memory long enough for the right structure to surface. One is concentrated
and exhausting, the other is diffuse and slow-burning. They're different
phenomenologically, but both feel like forms of deep engagement that are easy to
crowd out.

z3t4: I always search the web, ask others, or read books in order to find a
solution. When I do not find an answer from someone else, that's where I have to
think hard.

    soanvig: That's weird as I do the opposite: think by myself, then look for help
    if I don't know.

nunez: I will never not be upset at my fellow engineers for selling out the ONE
thing that made us valuable and respected in the marketplace and trying to
destroy software engineering as a career because "Claude Code go brrrrrr"
basically. It's like we had the means for production and more or less
collectively decided "You know what? Actually, the bourgeoisie can have it,
sure."

    oblio: Money. It's always money. It was always money.

        nunez: Couldn't agree more. AI as it's designed today is very heavy on the "f u;
        got mine" vibe.

    sph: The personification of the quote “your scientists were so preoccupied with
    whether or not they could that they didn't stop to think if they should” I feel
    the existential problem for a world that follows the religion of science and
    technology to its extreme, is that most people in STEM have no foundation in
    humanities, so ethical and philosophical concerns never pass through their mind.
    We have signed a pact with the devil to help us through boring tasks, and no one
    thought to ask what we would give in exchange.

LoganDark: Every time I try to use LLMs for coding, I completely lose touch with
what it's doing, it does everything wrong and it can't seem to correct itself no
matter how many times I explain. It's so frustrating just trying to get it to do
the right thing. I've resigned to mostly using it for "tip-of-my-tongue" style
queries, i.e. "where do I look in the docs". Especially for Apple platforms
where almost nothing is documented except for random WWDC video tutorials that
lack associated text articles. I don't trust LLMs at all. Everything they make,
I end up rewriting from scratch anyway, because it's always garbage. Even when
they give me ideas, they can't apply them properly. They have no standards, no
principle. It's all just slop. I hate this. I hate it because LLMs give so many
others the impression of greatness, of speed, and of huge productivity gains. I
must look like some grumpy hermit, stuck in their ways. But I just can't get
over how LLMs all give me the major ick. Everything that comes out of them feels
awful. My standards must be unreasonably high. Extremely, unsustainably high.
That must also be the reason I hardly finish any projects I've ever started, and
why I can never seem to hit any deadlines at work. LLMs just can't reach my
exacting, uncompromising standards. I'm surely expecting far too much of them.
Far too much. I guess I'll just keep doing it all myself. Anything else really
just doesn't sit right.

saturatedfat: I think for days at a time still. I don’t think you can get the
same satisfaction out of these tools if what you want to do is not novel. If you
are exploring the space of possibilities for which there are no clear solutions,
then you have to think hard. Take on wildly more ambitious projects. Try to do
something you don’t think you can do. And work with them to get there.

andyferris: My solution has been to lean into harder problems - even as side
projects, if they aren't available at work. I too am an ex-physcist used to
spending days thinking about things, but programming is a gold mine as it is
adjacent to computer science. You can design a programming language (or improve
an existing one), try to build a better database (or improve an existing one),
or many other things that are quite hard. The LLM is a good rubber duck for
exploring the boundaries of human knowledge (or at least knowledge common enough
to be in its training set). It can't really "research" on its own, and whenever
you suggest something novel and plausable it gets sycophantic, but it can help
you prototype ideas and implementation strategies quite fast, and it can help
you explore how existing software works and tackles similar problems (or help
you start working on an existing project).

dhananjayadr: The author's point is, If you use AI to solve the problem and
after the chat gives you the solution you say “oh yes, ok, I understand it, I
can do it”(and no, you can’t do it).

cranberryturkey: There's an irony here -- the same tools that make it easy to
skim and summarize can also be used to force deeper thinking. The problem isn't
the tools, it's the defaults. I've found that the best way to actually think
hard about something is to write about it, or to test yourself on it. Not re-
read it. Not highlight it. Generate questions from the material and try to
answer them from memory. The research on active recall vs passive review is
pretty clear: retrieval practice produces dramatically better long-term
retention than re-reading. Karpicke & Blunt (2011) showed that practice testing
outperformed even elaborative concept mapping. So the question isn't whether AI
summarizers are good or bad -- it's whether you use them as a crutch to avoid
thinking, or as a tool to compress the boring parts so you can spend more time
on the genuinely hard thinking.

userbinator: In my experience you will need to think even harder with AI if you
want a decent result, although the problems you'll be thinking about will be
more along the lines of "what the hell did it just write?" The current major
problem with the software industry isn't quantity, it's quality; and AI just
increases the former while decreasing the latter. Instead of e.g. finding ways
to reduce boilerplate, people are just using AI to generate more of it.

johanvts: I dont think LLMs really took away much thinking, for me they replaced
searching stackexchange to find incantations. Now I can get them instantly and
customized to my situation. I miss thinking hard too, but I dont blame that on
AI, its more that as a dev you are paid to think the absolute minimal amount
needed to solve an issue or implement a feature. I dont regret leaving academia,
but being paid to think I will always miss.

marcus_holmes: I think it's just another abstraction layer, and moves the
thinking process from "how do I solve this problem in code?" to "how do I solve
this problem in orchestration?". I recently used the analogy of when compilers
were invented. Old-school coders wrote machine code, and handled the intricacies
of memory and storage and everything themselves. Then compilers took over, we
all moved up an abstraction layer and started using high-level languages to code
in. There was a generation of programmers who hated compilers because they wrote
bad, inelegant, inefficient, programs. And for years they were right. The hard
problems now are "how can I get a set of non-deterministic, fault-prone, LLM
agents to build this feature or product with as few errors as possible, with as
little oversight as possible?". There's a few generic solutions, a few good
approaches coming out, but plenty of scope for some hard thought in there. And a
generic approach may not work for your specific project.

mightymosquito: While I see where you are coming from but I think what has
really gone for a toss is the utility of thinking hard. Thinking hard has never
been easier. I think AI for an autodidact is a boon. Now I suddenly have a
teacher who is always accessible and will teach me whatever I want for as long
as I want exactly the way I want and I don;t have to worry about my social
anxiety kicking in. Learn advanced cryptography? AI, figure out formal
verification - AI etc.

d--b: Why not think hard about what to build instead of how to build it?

rozumem: I can relate to this. Coding satisifies my urge to build and ship and
have an impact on the world. But it doesn't make me think hard. Two things which
I've recently gravitated to outside of coding which make me think: blogging and
playing chess. Maybe I subconsciously picked these up because my Thinker side
was starved for attention. Nice post.

tietjens: I wish the author would give some examples of what he wants to think
hard about.

jsattler: I had similar thoughts recently. I wouldn't consider myself "the
thinker", but I simply missed learning by failure. You almost don't fail anymore
using AI. If something fails, it feels like it's not your fault but the AI
messed up. Sometimes I even get angry at the AI for failing, not at myself. I
don't have a solution either, but I came up with a guideline on when and how to
use AI that has helped me to still enjoy learning. I'm not trying to advertise
my blog and you don't need to read it, the important part is the diagram at the
end of "Learning & Failure":
https://sattlerjoshua.com/writing/2026-02-01-thoughts-on-ai-.... In summary,
when something is important and long-term, I heavily invest into understanding
and use an approach that maximizes understanding over speed. Not sure if you can
translate it 100% to your situation but maybe it helps to have some kind of
guideline, when to spend more time thinking instead of directly using and AI to
get to the solution.

scionni: I have a very similar background and a very similar feeling when i
think of programming nowadays. Personally, I am going deeper in Quantum
Computing, hoping that this field will require thinkers for a long time.

yehoshuapw: have a look at  https://projecteuler.net/ for "Thinker" brain food.
(it still has the issue of not being a pragmatic use of time, but there are
plenty interesting enough questions which it at least helps)

dudeinjapan: If you feel this way, you arent using AI right. For me, Claude,
Suno, Gemini and AI tools are pure bliss for creation, because they eliminate
the boring grunt work. Who cares how to implement OAuth login flow, or anything
that has been done 1000 times? I do not miss doing grunt work!

smy20011: I miss entering flow state when coding. When vibe coding, you are in
constant interruption and only think very shallow. I never see anyone enter flow
state when vibe coding.

    krzat: Same here, waiting for response destroys any focus I have had.

    fragmede: The two ways I get into flow state these days are in setting up
    agentic loops, so I can get out of the way by letting AI check the results for
    itself, and by doing more things. I've got ~4 Claude Code instances working on
    problems, per project, and I've got multiple projects I'm working on at the same
    time.

fatfox: Just sit down and think hard. If it doesn’t work, think harder.

lxgr: I've had the completely opposite experience as somebody that also likes to
think more than to build: LLMs take much of the legwork of actually implementing
a design, fixing trivial errors etc. away from me and let me validate theories
much more quickly than I could do by myself. More importantly, thinking and
building are two very different modes of operating and it can be hard to switch
at moment's notice. I've definitely noticed myself getting stuck in "non-
thinking building/fixing mode" at times, only realizing that I've been making
steady progress into the wrong direction an hour or two in. This happens way
less with LLMs, as they provide natural time to think while they churn away at
doing. Even when thinking, they can help: They're infinitely patient rubber
ducks, and they often press all the right buttons of "somebody being wrong on
the Internet" too, which can help engineers that thrive in these kinds of verbal
pro/contra discussions.

BoostandEthanol: I’d been feeling this until quite literally yesterday, where I
sort of just forced myself to not touch an AI and grappled with the problem for
hours. Got myself all mixed up with trig and angles until I got a headache and
decided to back off a lot of the complexity. I doubt I got everything right, I’m
sure I could’ve had a solution with near identical outputs using an AI in a
fraction of the time. But I feel better for not taking the efficient way. Having
to be the one to make a decision at every step of the way, choosing the
constraints and where I cut my losses on accuracy, I think has taught me more
about the subject than even reading literature would’ve directly stated.

    repelsteeltje: I think the heart of the matter is this section in the blog: >
    Yes, I blame AI for this. > I am currently writing much more, and more
    complicated software than ever, yet I feel I am not growing as an engineer at
    all. [...] (emphasis added by me) AI is a force multiplier for accidental
    complexity in the Brooks sense. (https://en.wikipedia.org/wiki/No_Silver_Bullet)

practal: I see the current generation of AI very much as a thing in between.
Opus 4.5 can think and code quite well, but it cannot do these "jumps of
insight" yet. It also struggles with straightforward, but technically intricate
things, where you have to max out your understanding of the problem. Just a few
days ago, I let it do something that I thought was straightforward, but it kept
inserting bugs, and after a few hours of interaction it said itself it was
running in circles. It took me a day to figure out what the problem was: an
invariant I had given it was actually too strong, and needed to be weakened for
a special case. If I had done all of it myself, I would have been faster, and
discovered this quicker. For a different task in the same project I used it to
achieve a working version of something in a few days that would have taken me at
least a week or two to achieve on my own. The result is not efficient enough for
the long term, but for now it is good enough to proceed with other things. On
the other hand, with just one (painful) week more, I would have coded a proper
solution myself. What I am looking forward to is being able to converse with the
AI in terms of a hard logic. That will take care of the straightforward but
technically intricate stuff that it cannot do yet properly, and it will also
allow the AI to surface much quicker where a "jump of insight" is needed. I am
not sure what all of this means for us needing to think hard. Certainly thinking
hard will be necessary for quite a while. I guess it comes down to when the AIs
will be able to do these "jumps of insight" themselves, and for how long we can
jump higher than they can.

capl: That’s funny cause I feel the opposite: LLMs can automate, in a sloppy
fashion, building the first trivial draft. But what remains is still thinking
hard about the non trivial parts.

7777332215: Seems like a lot of people use AI to code in their private
commercial IP and products. How are people not concerned with the fact that all
these ai companies have the source code to everything? Your just helping them
destroy your job. Code is not worthless, you cannot easily duplicate any complex
project with equal features, quality, and stability.

    practal: I think that is a very good point. Code is definitely not worthless,
    but I don't think that capitalism has the right tools for pricing it properly. I
    think it will become a lot like mathematics in that way.

msephton: I'm not sure I agree. Actually, I don't agree. You only stop thinking
hard if you decide to stop thinking hard. Nobody, no tool, is forcing you to
stop thinking, pushing, reaching. If the thinking ceiling has changed, which I
think it has, then it's entirely up to you to either move with it or stay still.

m0rc: I think the article has a point. There seem to be two reactions among
senior engineers atound me these days. On one side, there are people who have
become a bit more productive. They are certainly not "10x," but they definitely
deliver more code. However, I do not observe a substantial difference in the
end-to-end delivery of production-ready software. This might be on me and my
lack of capacity to exploit the tools to their full extent. But, iterating over
customer requirements, CI/CD, peer reviews, and business validation takes time
(and time from the most experienced people, not from the AI). On the other hand,
soemtimes I observe a genuine degradation of thinking among some senior
engineers (there aren’t many juniors around, by the way). Meetings,
requirements, documents, or technology choices seem to be directly copy/pasted
from an LLM, without a grain of original thinking, many times without insight.
The AI tools are great though. They give you an answer to the question. But,
many times making the correct question, and knowing when the answer is not
correct is the main issue. I wonder if the productivity boost that senior
engineers actually need is to profit from the accumulated knowledge found in
books. I know it is an old technology and it is not fashionable, but I believe
it is mostly unexploited if you consider the whole population of engineers :D

ertucetin: It’s the journey, not the destination, but with AI it’s only the
destination, and it takes all the joy.

thorum: > but the number of problems requiring deep creative solutions feels
like it is diminishing rapidly. If anything, we have more intractable problems
needing deep creative solutions than ever before. People are dying as I write
this. We’ve got mass displacement, poverty, polarization in politics. The
education and healthcare systems are broken. Climate change marches on. Not to
mention the social consequences of new technologies like AI (including the ones
discussed in this post) that frankly no one knows what to do about. The solution
is indeed to work on bigger problems. If you can’t find any, look harder.

lccerina: "Oh no, I am using a thing that no one is forcing me to use, and now I
am sad". Just don't use AI. The idea that you have ship ship ship 10X ship is an
illusion and a fraud. We don't really need more software

tomquirk: The answer to this is to shift left into product/design. Sure, I'm
doing less technical thinking these days. But all the hard thinking is happening
on feature design. Good feature design is hard for AI. There's a lot of hidden
context: customer conversations, unwritten roadmaps, understanding your users
and their behaviour, and even an understanding of your existing feature set and
how this new one fits in. It's a different style of thinking,  but it is hard,
and a new challenge we gotta embrace imo.

    margorczynski: > Good feature design is hard for AI For now. Go back a year and
    take a look how the AI/LLM coding tools looked and worked back then.

foxes: I think I miss my thinking..

defraudbah: another AI blame/praise/adapt..  you definitely didn't think hard
about this one, did you

charcircuit: If you are thinking hard I think you are software engineering
wrong. Even before AI. As an industry all the different ways of doing things
have already played out. Even doing big reactors or performance optimizations
often can not be 100% predicted in their effectiveness. You will want to just go
ahead and implement these things over spending more time thinking. And as AI
gets stronger the just try a bunch of approaches will beat the think hard
approach by an even bigger margin.

    globular-toast: Why do anything at all then? It's all been done before. This
    line of thinking sounds like depression to me. Why decorate my house? I know I
    could do it, but it's all been done before, why bother?

        charcircuit: Just because we know the best way to add 2 ints together that
        doesn't mean it's pointless to do addition with ints. We don't need people
        trying to spend a lot of extra time to come up with alternate ways to do it. The
        right function using addition may be valuable to a certain population.

bowsamic: I specifically spend my evenings reading Hegel and other hard
philosophy as well as writing essays just to force myself to think hard

ccppurcell: In my experience, the so-called 1% are mostly just thinkers and
researchers who have dedicated a lot more time from an earlier age to thinking
and/or researching. There are a few geniuses out there but it's 1 in millions
not in hundreds.

tbs1980: I read something similar here
https://open.substack.com/pub/strangeloopcanon/p/on-thinkers...

hpone91: Just give Umineko a play/readthrough to get your deep thinking gray
cells working again.

cladopa: I believe the article is wrong in so many ways. If you think too much
you get into dead ends and you start having circular thoughts, like when you are
lost in the desert and you realise you are in the same place again after two
hours as you have made a great circle(because one of your legs is dominant over
the other). The thinker needs feedback on the real world. It needs constant
testing of hypothesis on reality or else you are dealing with ideology, not
critical thinking. It needs other people and confrontation of ideas so the ideas
stay fresh and strong and do not stagnate in isolation and personal biases. That
was the most frustrating thing before AI, a thinker could think very fast, but
was limited in testing by the ability to build. Usually she had to delegate it
to people that were better builders, or else she had to be builder herself,
doing what she hates all the time.

macmac_mac: reading this made me realize i used to actually think hard about
bugs and design tradeoffs because i had no choice

voidUpdate: If you miss the experience of not using LLMs, then just... don't? Is
someone forcing you to code with LLM help?

    globular-toast: I think a lot of people are struggling with this. Look at the
    obesity epidemic. Nobody is forcing you to buy ultraprocessed foods. Nobody is
    forcing you to overeat. You can still cook with fresh vegetables at home. But
    many/most people in Western countries struggle with their weight. An even better
    analogy is the slot machine. Once you've "won" one time it's hard to break the
    cycle. There's so little friction to just having another spin. Everyone needs to
    go and see the depressed people at slot machines at least once to understand
    where this ends.

freshbreath: "I don't want to have to write this for the umpteenth time" --
Don't let it even reach a -teenth. Automate it on the 2nd iteration. Or even the
1st if you know you'll need it again. LLMs can help with this. Software
engineers are lazy. The good ones are, anyway. LLMs are extremely dangerous for
us because it can easily become a "be lazy button". Press it whenever you want
and get that dopamine hit -- you don't even have to dive into the weeds and get
dirty! There's a fine line between "smart autocomplete" and "be lazy button".
Use it to generate a boilerplate class, sure. But save some tokens and fill that
class in yourself. Especially if you don't want to (at your own discretion;
deadlines are a thing). But get back in those weeds, get dirty, remember the
pain. We need to constantly remind ourselves of what we are doing and why we are
doing it. Failing that, we forget the how, and eventually even the why. We
become the reverse centaur. And I don't think LLMs are the next layer of
abstraction -- if anything, they're preventing it. But I think LLMs can help
build that next layer... it just won't look anything like the weekly "here's the
greatest `.claude/.skills/AGENTS.md` setup". If you have to write a ton of
boilerplate code, then abstract away the boilerplate in code (nondeterminism is
so 2025). And then reuse that abstraction. Make it robust and thoroughly tested.
Put it on github. Let others join in on the fun. Iterate on it. Improve it.
Maybe it'll become part of the layers of abstraction for the next generation.

tevli: Exactly what I've been thinking. outsourcing tasks and thinking of
problems to AI just seems easier these days; and you still get to feel in charge
because you're  the one still giving instructions.

kovkol: I mean I spent most of my career been being pressured to move from type
3 to any one of the other 2 so I don't blame AI for this (it doesn't help,
though, especially if you delegate to much to it).

zepesm: That's why i'm still pushing bytes on C64 demoscene (and recommend such
a niche as a hobby to anyone). It's great for the sanity in modern ai-driven
dev-world ;)

yieldcrv: man, setting up worktrees for parallelized agentic coding is hard,
setting up containerized worktrees is hard so you can run with dangerous
permissions on without nuking host system deciding whether to use that to work
on multiple features on the same code base, or the same feature in multiple
variations is hard deciding whether to work on a separate project entirely while
all of this is happening is hard and mentally taxing planning all of this up for
a few hours and watching it go all at once autonomously is satisfying!

muyuu: this also used to happened to me when I in a position that involved a lot
of research earlier on and then after the product was a reality, and it worked,
it tapered off to be small improvements and maintenance I can imagine many
positions work out this way in startups it's important to think hard sometimes,
even if it means taking time off to do the thinking - you can do it without the
socioeconomic pressure of a work environment

noodleweb: I miss this too, I have had those moments of reward where something
works and I want to celebrate. It's missing too for me. With AI the pros
outweigh the cons at least at the moment with what we collectively have figured
out so far. But with that everyday I wonder if it's possible now to be more
ambitious than ever and take on much bigger problem with the pretend smart
assistant.

martin1975: I've been writing C/C++/Java for 25 years and am trying to learn
forex disciplined, risk managed forex trading, It's a whole new level of hard
work/thinking.

jillesvangurp: You can't change the world, you can change yourself. Many people
don't like change. So, people get frustrated when the world inevitably changes
and they fail to adapt. It's called getting older. Happens to us all. I'm not
immune to that and I catch myself sometimes being more reluctant to adapt. I'm
well aware and I actively try to force myself to adapt. Because the alternative
is becoming stuck in my ways and increasingly less relevant. There are a lot of
much younger people around me that still have most of their careers ahead of
them. They can try to whine about AI all they want for the next four decades or
so but I don't think it will help them. Or they can try to deal with the fact
that these tools are here now and that they need to learn to adapt to them
whether they like it or not. And we are probably going to see quite some
progress on the tool front. It's only been 3 years since ChatGPT had its public
launch. To address the core issue here. You can use AI or let AI use you. The
difference here is about who is in control and who is setting the goals. The
traditional software development team is essentially managers prompting
programmers to do stuff. And now we have programmers prompting AIs to do that
stuff. If you are just a middle man relaying prompts from managers to the AI,
you are not adding a lot of value. That's frustrating. It should be because it
means apparently you are very replaceable. But you can turn that around. What
makes that manager the best person to be prompting you? What's stopping them
from skipping that entirely? Because that's your added value. Whatever you are
good at and they are not is what you should be doing most of your time. The AI
tools are just a means to an end to free up more time for whatever that is.
Adapting means figuring that out for yourself and figuring out things that you
enjoy doing that are still valuable to do. There's plenty of work to be done.
And AI tools won't lift a finger to do it until somebody starts telling them
what needs doing. I see a lot of work around me that isn't getting done. A lot
of people are blind to those opportunities. Hint: most of that stuff still looks
like hard work. If some jerk can one shot prompt it, it isn't all that valuable
and not worth your time. Hard work usually involves thinking hard, skilling up,
and figuring things out. The type of stuff the author is complaining he misses
doing.

petterroea: I've missed the same even since before AI because I've done far too
much work that's simple but time intensive. It's frustrating, and I miss
problems that keep me up all night. Reverse engineering is imo the best way of
getting the experience of pushing your thinking in a controlled way, at least if
you have the kind of personality where you are stubborn in wanting to solve the
problem. Go crack an old game or something!

Underqualified: This resonates with me, but I quit programming about a decade
ago when we were moving from doing low level coding to frameworks. It became no
longer about figuring out the actual problem, but figuring out how to get the
framework to solve it and that just didn't work for me. I do miss hard thinking,
I haven't really found a good alternative in the meantime. I notice I get joy
out of helping my kids with their, rather basic, math homework, so the part of
me that likes to think and solve problems creatively is still there. But it's
hard to nourish in today's world I guess, at least when you're also a 'builder'
and care about efficiency and effectiveness.

thegrim000: You know, I was expecting what the post would say and was prepared
to dunk on it and just tell them to stop using ai then, but the builder/thinker
division they presented got me thinking. How ai/vibe coding fulfills the
builder, not the thinker, made me realize that I'm basically 100% thinker, 0%
builder, and that's why I don't really care at all about ai for coding. I'll
spend years working on a from scratch OS kernel or a vulkan graphics engine or
whatever other ridiculous project, which never sees the light of day, because I
just enjoy the thinking / hard work. Solving hard problems is my entertainment
and my hobby. It's cool to eventually see results in those projects, but that's
not really the point. The point is to solve hard problems. I've spent decades on
personal projects that nobody else will ever see. So I guess that explains why I
see all the ai coding stuff and pretty much just ignore it. I'll use ai now as
an advanced form of google, and also as a last ditch effort to get some
direction on bugs I truly can't figure out, but otherwise I just completely
ignore it. But I guess there's other people, the builders, where ai is a
miraculous thing and they're going to crazy lengths to adopt it in every
workflow and have it do as much as possible. Those 'builder' types of people are
just completely different from me.

Bengalilol: Cognitive debt lies ahead for all of us.

jurgenaut23: Man, this resonates SO MUCH with me. I have always loved being
confronted with a truly difficult problem. And I always had that (obviously
misguided, but utterly motivating) feeling that, with enough effort, no problem
could ever resist me. That it was just a matter of grinding a bit further, a bit
longer. This is why I am so deeply opposed to using AI for problem solving I
suppose: it just doesn’t play nice with this process.

Thanemate: I am one of those junior software developers who always struggled
with starting their own projects. Long story short, I realized that my struggle
stems from my lack of training in open-ended problems, where there are many ways
to go about solving something, and while some ways are better than others,
there's no clear cut answer because the tradeoffs may not be relevant with the
end goal. I realized that when a friend of mine gave me Factorio as a gift last
Christmas, and I found myself facing the exact same resistance I'm facing while
thinking about working on my personal projects. To be more specific, it's a fear
and urge of closing the game and leaving it "for later" the moment I discover
that I've either done something wrong or that new requirements have been added
that will force me to change the way my factories connect with each other (or
even their placement). Example: Tutorial 4 has the players introduced to
research and labs, and this feeling appears when I realize that green science
requires me to introduce all sorts of spaghetti just to create the mats needed
for green science! So I've done what any AI user would do and opted to use
chatGPT to push through the parts where things are either overwhelming,
uncertain, too open-ended, or everything in between. The result works, because
the LLM has been trained to Factorio guides, and goes as far as suggesting
layouts to save myself some headache! Awesome, no? Except all I've done is
outsource the decision of how to go about "the thing" to someone else. And while
true, I could've done this even before LLM's by simply watching a youtube video
guide, the LLM help doesn't stop there: It can alleviate my indecisiveness and
frustration with dealing with open-ended problems for personal projects, can
recommend me project structure, can generate a bullet pointed lists to pretend
that I work for a company where someone else creates the spec and I just follow
it step by step like a good junior software engineer would do. And yet all I did
just postponed the inevitable exercise of a very useful mental habit: To
navigate uncertainty, pause and reflect, plan, evaluate a trade-off or 2 here
and there. And while there are other places and situations where I can exercise
that behavior, the fact remains that my specific use of LLM removed that weight
off my shoulders. I became objectively someone who builds his project ideas and
makes progress in his Factorio playthrough, but the trade-off is I remain the
same person who will duck and run the moment resistance happens, and succumb to
the urge of either pushing "the thing" for tomorrow or ask chatGPT for help. I
cannot imagine how someone would claim that removing an exercise from my daily
gym visit will not result in weaker muscles. There are so many hidden
assumptions in such statements, and an excessive focus of results in "the new
era where you should start now or be left behind" where nobody's thinking how
this affects the person and how they ultimately function in their daily lives
across multiple contexts. It's all about output, output, output. How far are we
from the day where people will say "well, you certainly don't need to plan a
project, a factory layout, or even decide, just have chatGPT summarize the
trade-offs, read the bullet points, and choose". We're off-loading portion of
the research AND portion of the execution, thinking we'll surely be activating
the neurosynapses in our brains that retains habits, just like someone who lifts
50% lighter weights at the gym will expect to maintain muscle mass or burn fat.

whywhywhywhy: I feel tired working with AI much faster than I did when I used to
code, dunno if it's just that I don't really need to think much at all other
than keep in mind the broad plan and have an eye out if a red flag of the wrong
direction shows in the transcript, don't even bother reading the code anymore
since Opus 4.5 I haven't felt the need to. Manually coding engaged my brain much
more and somehow was less exhausting, kinda feels like getting out of bed and
doing something vs lazing around and ending up feel more tired despite having to
do less.

    jurgenaut23: Something that people underestimate a lot is that we aren’t “brains
    in a jar” and the elevated states of consciousness, such as “flow”, require a
    deep involvement of the body. As such manual coding is much more likely to bring
    you into the zone than irregular interactions with an LLM. I actually believe
    that there are much better ways to incorporate AI into software development than
    any of the mechanisms we’ve seen so far. For instance, it would make a lot more
    sense that you actually write the software manually and get the usual
    autocomplete suggestions, along with some on the fly reviews, an extension
    proposals, such as writing the body of a function that you’re calling from the
    core function you’re writing now.

Meneth: I knew this sort of thing would happen before it was popular.
Accordingly: Never have I ever used an LLM.

frgturpwd: It seems like what you miss is actually a stable cognitive regime
built around long uninterrupted internal simulation of a single problem. This is
why people play strategy video games.

nubinetwork: > the number of times I truly ponder a problem for more than a
couple of hours has decreased tremendously Isn't that a good thing?  If you're
stuck on the same problem forever, then you're not going to get past it and
never move on to the next thing... /shrug

enthus1ast_: When I wrote nimja's template inheritance. I thought about it
multiple days, until, during a train commute, it made click and I had to get out
my notebook and write it, directly in the train. Then some month later I found
out, I had the same bug that jinja2 had fixed years ago. So I felt kinda like a
brothers in hard thinking :)

est: I wrote a blog about this as well Hard Things in Computer Science, and AI
Aren't Fixing Them https://blog.est.im/2026/stderr-04

cbdevidal: It’s possible to be both. The last time I had to be a Thinker was
because I was in Builder mode. I’ve been trying to build an IoT product but I’ve
been wayyyy over my head because I knew maybe 5% of what I needed to be
successful. So I would get stuck many, many times, for days or weeks at a time.
I will say though that AI has made the difference in the last few times I got
stuck. But I do get more enjoyment out of Building than Thinking, so I embrace
it.

fattybob: Thinking hard and fast with positive results is like a drug, ah those
were good and rewarding days in my past, would jump back into that work
framework any time ( that was running geological operations in an unusually
agile oil exploration programme )
